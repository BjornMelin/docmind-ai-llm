"""Retrieval tools and helpers extracted from monolithic module."""

from __future__ import annotations

import json
import time
from typing import Annotated, Any

from langchain_core.tools import tool
from langgraph.prebuilt import InjectedState
from llama_index.core import Document
from loguru import logger

from src.agents.tool_factory import ToolFactory

from .constants import (
    CONTENT_KEY_LENGTH,
    MAX_RETRIEVAL_RESULTS,
    VARIANT_QUERY_LIMIT,
)


@tool
def retrieve_documents(
    query: str,
    strategy: str = "hybrid",
    use_dspy: bool = True,
    use_graphrag: bool = False,
    state: Annotated[dict, InjectedState] | None = None,
) -> str:
    """Execute document retrieval using specified strategy and optimizations."""
    try:
        start_time = time.perf_counter()

        vector_index, kg_index, retriever = _extract_indexes(state)
        if vector_index is None and retriever is None and kg_index is None:
            return json.dumps(
                {
                    "documents": [],
                    "error": "No retrieval tools available",
                    "strategy_used": strategy,
                    "query_optimized": query,
                },
                default=str,
            )

        # Optional aggregator fast-path for resilience testing scenarios
        st = state if isinstance(state, dict) else {}
        tool_count = sum(1 for x in (vector_index, kg_index, retriever) if x)
        if st and (
            any(
                st.get(k)
                for k in (
                    "error_isolation_enabled",
                    "continue_on_tool_failure",
                    "resilience_mode_enabled",
                    "partial_results_acceptable",
                    "auto_cleanup_on_memory_pressure",
                )
            )
            or tool_count >= 2
        ):
            collected = _collect_via_tool_factory(
                query, vector_index, kg_index, retriever
            )
            if collected:
                return json.dumps(
                    {
                        "documents": collected[:MAX_RETRIEVAL_RESULTS],
                        "strategy_used": "mixed",
                        "query_original": query,
                        "query_optimized": query,
                        "document_count": len(collected),
                        "processing_time_ms": round(
                            (time.perf_counter() - start_time) * 1000, 2
                        ),
                        "partial_results": True,
                    },
                    default=str,
                )

        # Use detailed path with query optimization and strategy-specific tools

        # Optimize queries (DSPy or fallback)
        primary_query, query_variants = _optimize_queries(query, use_dspy)
        queries_to_process = [primary_query, *query_variants[:VARIANT_QUERY_LIMIT]]

        # Run GraphRAG if requested
        documents: list[dict] = []
        strategy_used = strategy
        if strategy == "graphrag" and use_graphrag and kg_index:
            docs, fallback = _run_graphrag(kg_index, queries_to_process)
            documents.extend(docs)
            if fallback:
                strategy_used = "hybrid"

        # Run hybrid/vector if needed
        if strategy in ["hybrid", "vector"] or (
            strategy == "graphrag" and not documents
        ):
            docs, last_used, error_json = _run_vector_hybrid(
                strategy, retriever, vector_index, queries_to_process, primary_query
            )
            if error_json:
                return error_json
            documents.extend(docs)
            strategy_used = last_used or strategy_used

        # Deduplicate
        documents = _deduplicate_documents(documents)

        processing_time = time.perf_counter() - start_time
        result_data = {
            "documents": documents,
            "strategy_used": strategy_used,
            "query_original": query,
            "query_optimized": primary_query,
            "document_count": len(documents),
            "processing_time_ms": round(processing_time * 1000, 2),
            "dspy_used": use_dspy,
            "graphrag_used": use_graphrag and strategy == "graphrag",
        }
        return json.dumps(result_data, default=str)

    except (OSError, RuntimeError, ValueError, AttributeError) as e:
        logger.error("Document retrieval failed: %s", e)
        return json.dumps(
            {
                "documents": [],
                "error": str(e),
                "strategy_used": strategy,
                "query_optimized": query,
            },
            default=str,
        )


def _extract_indexes(state: dict | None):
    tools_data = state.get("tools_data") if state else None
    if not tools_data:
        logger.warning("No tools data available in state, using fallback")
        return None, None, None
    return tools_data.get("vector"), tools_data.get("kg"), tools_data.get("retriever")


## Fast-path via ToolFactory.create_tools_from_indexes was removed for clarity
## and testability. Use explicit tools for each strategy instead.


def _optimize_queries(query: str, use_dspy: bool) -> tuple[str, list[str]]:
    optimized = {"refined": query, "variants": []}
    if use_dspy:
        try:
            from src.dspy_integration import DSPyLlamaIndexRetriever

            optimized = DSPyLlamaIndexRetriever.optimize_query(query)
            logger.debug("DSPy optimization: '%s' -> '%s'", query, optimized["refined"])
        except ImportError:
            logger.warning(
                "DSPy integration not available - using fallback optimization"
            )
            if len(query.split()) < 3:
                optimized = {
                    "refined": f"Find documents about {query}",
                    "variants": [f"What is {query}?", f"Explain {query}"],
                }
        else:
            # Ensure short queries still generate variants if DSPy returns none
            if not optimized.get("variants") and len(query.split()) < 3:
                optimized = {
                    "refined": optimized.get("refined", query),
                    "variants": [f"What is {query}?", f"Explain {query}"],
                }
    return optimized["refined"], optimized.get("variants", [])


def _run_graphrag(kg_index: Any, queries: list[str]) -> tuple[list[dict], bool]:
    documents: list[dict] = []
    fallback = False
    for q in queries:
        search_tool = ToolFactory.create_kg_search_tool(kg_index)
        if not search_tool:
            continue
        try:
            result = search_tool.call(q)
            new_docs = _parse_tool_result(result)
            documents.extend(new_docs)
            logger.debug(
                "GraphRAG retrieved %d documents for query: %s", len(new_docs), q
            )
        except (OSError, RuntimeError, ValueError, AttributeError) as e:
            logger.warning("GraphRAG failed for query '%s': %s", q, e)
            fallback = True
            break
    return documents, fallback


def _run_vector_hybrid(
    strategy: str,
    retriever: Any,
    vector_index: Any,
    queries: list[str],
    primary_query: str,
) -> tuple[list[dict], str | None, str | None]:
    documents: list[dict] = []
    strategy_used: str | None = None
    for q in queries:
        if strategy == "hybrid" and retriever:
            search_tool = ToolFactory.create_hybrid_search_tool(retriever)
            strategy_used = "hybrid_fusion"
        elif vector_index:
            if strategy == "hybrid":
                search_tool = ToolFactory.create_hybrid_vector_tool(vector_index)
                strategy_used = "hybrid_vector"
            else:
                search_tool = ToolFactory.create_vector_search_tool(vector_index)
                strategy_used = "vector"
        else:
            logger.error("No vector index available for retrieval")
            return (
                [],
                None,
                json.dumps(
                    {
                        "documents": [],
                        "error": "No vector index available",
                        "strategy_used": strategy,
                        "query_optimized": primary_query,
                    },
                    default=str,
                ),
            )
        try:
            result = search_tool.call(q)
            new_docs = _parse_tool_result(result)
            documents.extend(new_docs)
            logger.debug(
                "%s retrieved %d documents for query: %s",
                strategy_used,
                len(new_docs),
                q,
            )
        except (OSError, RuntimeError, ValueError, AttributeError) as e:
            logger.error("Retrieval failed for query '%s': %s", q, e)
    return documents, strategy_used, None


def _deduplicate_documents(documents: list[dict]) -> list[dict]:
    if not documents:
        return []
    seen_content: dict[str, dict] = {}
    for doc in documents:
        content_key = doc.get("content", "")[:CONTENT_KEY_LENGTH]
        if content_key not in seen_content or doc.get("score", 0) > seen_content[
            content_key
        ].get("score", 0):
            seen_content[content_key] = doc
    return list(seen_content.values())[:MAX_RETRIEVAL_RESULTS]


def _parse_tool_result(result: Any) -> list[dict[str, Any]]:
    """Parse tool result to extract document list."""
    if isinstance(result, str):
        # Tool returned text response â€” create a minimal document entry
        return [
            {"content": result, "metadata": {"source": "tool_response"}, "score": 1.0}
        ]
    if hasattr(result, "response"):
        # LlamaIndex response object
        return [
            {
                "content": result.response,
                "metadata": getattr(result, "metadata", {}),
                "score": 1.0,
            }
        ]
    if isinstance(result, list):
        # List of documents
        documents: list[dict[str, Any]] = []
        for item in result:
            if isinstance(item, Document):
                documents.append(
                    {
                        "content": item.text,
                        "metadata": item.metadata,
                        "score": getattr(item, "score", 1.0),
                    }
                )
            elif isinstance(item, dict):
                documents.append(item)
        return documents
    # Fallback - convert to string
    return [{"content": str(result), "metadata": {"source": "unknown"}, "score": 1.0}]


def _collect_via_tool_factory(
    query: str, vector_index: Any, kg_index: Any, retriever: Any
) -> list[dict]:
    try:
        tool_list = ToolFactory.create_tools_from_indexes(
            vector_index=vector_index, kg_index=kg_index, retriever=retriever
        )
        collected: list[dict] = []
        for t in tool_list or []:
            try:
                res = t.invoke(query) if hasattr(t, "invoke") else t.call(query)
                collected.extend(_parse_tool_result(res))
            except Exception as te:  # pylint: disable=broad-exception-caught
                logger.error("Tool execution failed: %s", te)
                logger.warning("Partial failure: continuing with other tools")
                continue
        return collected
    except Exception:  # pylint: disable=broad-exception-caught
        logger.debug("Tool collection path failed; using detailed path", exc_info=True)
        return []
