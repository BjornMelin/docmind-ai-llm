# Delta Specification: Retrieval System Enhancements

## Metadata

- **Feature ID**: FEAT-002.1
- **Parent Spec**: FEAT-002 (002-retrieval-search.spec.md)
- **Version**: 1.0.0
- **Status**: Ready for Implementation
- **Created**: 2025-01-21
- **Updated**: 2025-01-21
- **Type**: Delta Specification (Completion of FEAT-002)
- **ADR Dependencies**: [ADR-002, ADR-004, ADR-010, ADR-018, ADR-019]
- **Implementation Status**: 0% (Not Started)
- **Requirements Covered**: REQ-0044, REQ-0049, REQ-0050 (completion)
- **Test Coverage Target**: 40%+ for all retrieval modules

## 1. Objective

This delta specification completes the FEAT-002 Retrieval & Search System using a **library-first approach** that leverages existing LlamaIndex capabilities rather than building custom implementations. It configures and optimizes the native CLIP embeddings, PropertyGraphIndex, and DSPy features already available in the ecosystem, achieving 100% completion of all FEAT-002 requirements with 70% less custom code.

### Current State (from FEAT-002)

- **BGE-M3**: âœ… Fully implemented with ColBERT support enabled
- **RouterQueryEngine**: âœ… Adaptive routing operational
- **CrossEncoder**: âœ… BGE-reranker-v2-m3 working
- **CLIP Embeddings**: âŒ 0% - Missing integration (REQ-0044)
- **PropertyGraphIndex**: ðŸŸ¡ 25% - Needs configuration with domain extractors (REQ-0049)
- **DSPy Optimization**: ðŸŸ¡ 25% - Basic query expansion only (REQ-0050)
- **vLLM FP8**: âŒ Using mocks instead of real FP8 optimization

### Target State (after FEAT-002.1)

- **CLIP Embeddings**: âœ… 100% - LlamaIndex ClipEmbedding with 1.4GB VRAM constraint
- **PropertyGraphIndex**: âœ… 100% - Configured LlamaIndex PropertyGraphIndex with domain extractors
- **DSPy Optimization**: âœ… 100% - Progressive zero-shot â†’ few-shot â†’ production optimization with >20% quality improvement
- **vLLM FP8**: âœ… 100% - Native FP8 quantization on RTX 4090 (Ada Lovelace)
- **Test Coverage**: âœ… 40%+ for all retrieval modules

## 2. Scope

### In Scope

#### Multimodal Support (REQ-0044)

- Configure LlamaIndex ClipEmbedding for ViT-B/32 model
- Integrate with existing Qdrant collections for image embeddings
- Update RouterQueryEngine for multimodal query classification
- Enable cross-modal similarity search (text-to-image, image-to-image)
- Validate 1.4GB VRAM constraint compliance

#### PropertyGraphIndex (REQ-0049)

- Configure existing LlamaIndex PropertyGraphIndex with domain-specific extractors
- Use built-in SimpleLLMPathExtractor and ImplicitPathExtractor
- Tune entity and relationship extraction for technical documentation
- Integrate SimplePropertyGraphStore with existing infrastructure
- Configure multi-hop traversal with depth limits (default: 2)
- Test hybrid vector+graph retrieval performance

#### DSPy Optimization (REQ-0050)

- Implement progressive DSPy optimization strategy:
  - **Phase 1**: Zero-shot MIPROv2 optimization (no training data required)
  - **Phase 2**: Few-shot learning with 5-10 example queries
  - **Phase 3**: Production optimization with accumulated user data
- Use LlamaIndexRM for seamless retriever integration
- Implement query variant generation and scoring
- Validate >20% quality improvement with A/B testing

#### vLLM FP8 Optimization (ADR-010)

- Configure vLLM with native FP8 quantization (RTX 4090 Ada Lovelace supports FP8)
- Enable FP8 KV cache with `kv_cache_dtype="fp8_e5m2"`
- Use FlashInfer attention backend via `VLLM_ATTENTION_BACKEND=FLASHINFER`
- Validate 128K context window with 50% memory reduction
- Profile performance: 120-180 tokens/sec decode, 900-1400 tokens/sec prefill

### Out of Scope

- Custom embedding model training
- Distributed graph processing
- Real-time index updates (remains batch-based)
- Cross-lingual capabilities

## 3. Inputs and Outputs

### Inputs

#### Multimodal Inputs

- **Image Files**: JPEG, PNG, WebP (max 10MB per image)
- **Image Batch Size**: Up to 32 images per request
- **Mixed Queries**: Text + image combinations for hybrid search
- **Image Preprocessing**: Auto-resize to 224x224 for CLIP

#### Graph Inputs

- **Entity Types**: PERSON, ORG, PRODUCT, LOCATION, DATE, TECH
- **Relationship Types**: USES, MENTIONS, RELATED_TO, PART_OF
- **Traversal Depth**: Configurable 1-3 hops (default: 2)
- **Graph Filters**: Entity type, relationship type, confidence scores

#### DSPy Inputs

- **Query Types**: Factual, analytical, relationship, comparison
- **Optimization Mode**: Bootstrap, few-shot, or zero-shot
- **Examples**: 20+ labeled query-document pairs for bootstrap
- **Metrics**: NDCG@10, MRR, precision@k for validation

### Outputs

#### Enhanced Outputs

- **Multimodal Results**: Documents with text and image relevance scores
- **Graph Context**: Entity relationships and connection paths
- **Optimized Queries**: Original + 3-5 rewritten variants with scores
- **Performance Metrics**: Latency, quality improvement %, resource usage
- **Explanation**: Why specific strategies were selected

## 4. Interfaces

### CLIP Multimodal Configuration (Library-First)

```python
from llama_index.embeddings.clip import ClipEmbedding
from llama_index.core import VectorStoreIndex, Settings
from llama_index.core.schema import ImageDocument

# Simple CLIP configuration using existing LlamaIndex components
clip_embedding = ClipEmbedding(
    model_name="openai/clip-vit-base-patch32",
    embed_batch_size=10,  # Optimize for 1.4GB VRAM constraint
    device="cuda"
)

# Configure multimodal index
Settings.embed_model = clip_embedding

# Usage for images
image_docs = [
    ImageDocument(image_path="/path/to/image.jpg"),
    # ... more images
]

# Create index with both text and images
from llama_index.core import MultiModalVectorStoreIndex
index = MultiModalVectorStoreIndex.from_documents(
    documents + image_docs,
    storage_context=storage_context  # Existing Qdrant setup
)

# Cross-modal search
query_engine = index.as_query_engine(
    similarity_top_k=5,
    image_similarity_top_k=3
)

# Validate VRAM usage
import torch
vram_gb = torch.cuda.memory_allocated() / 1024**3
assert vram_gb < 1.4, f"VRAM usage {vram_gb:.2f}GB exceeds 1.4GB limit"
```

### PropertyGraphIndex Configuration (Library-First)

```python
from llama_index.core import PropertyGraphIndex
from llama_index.core.indices.property_graph import (
    SimpleLLMPathExtractor,
    ImplicitPathExtractor,
    SchemaLLMPathExtractor
)
from llama_index.core.indices.property_graph.utils import (
    default_parse_triplets_fn
)

# Configure domain-specific entity extraction
tech_schema = {
    "entities": ["FRAMEWORK", "LIBRARY", "MODEL", "HARDWARE", "PERSON", "ORG"],
    "relations": ["USES", "OPTIMIZED_FOR", "PART_OF", "CREATED_BY", "SUPPORTS"]
}

# Use existing LlamaIndex extractors with domain tuning
kg_extractors = [
    # LLM-based extraction for technical relationships
    SimpleLLMPathExtractor(
        llm=Settings.llm,
        max_paths_per_chunk=20,
        num_workers=4
    ),
    
    # Schema-guided extraction for consistent entity types
    SchemaLLMPathExtractor(
        llm=Settings.llm,
        possible_entities=tech_schema["entities"],
        possible_relations=tech_schema["relations"],
        kg_validation_schema=tech_schema,
        strict=True
    ),
    
    # Implicit relationship extraction from structure
    ImplicitPathExtractor()
]

# Create PropertyGraphIndex with existing infrastructure
from llama_index.core.storage.property_graph import SimplePropertyGraphStore

property_graph_store = SimplePropertyGraphStore()
index = PropertyGraphIndex.from_documents(
    documents,
    kg_extractors=kg_extractors,
    property_graph_store=property_graph_store,
    vector_store=vector_store,  # Existing Qdrant
    show_progress=True,
    max_triplets_per_chunk=10
)

# Query with built-in retrieval
retriever = index.as_retriever(
    include_text=False,  # Graph-only retrieval
    similarity_top_k=10,
    path_depth=2  # Multi-hop traversal
)

# Hybrid vector + graph query engine
query_engine = index.as_query_engine(
    retriever_mode="hybrid",  # Combine vector + graph
    response_mode="tree_summarize",
    verbose=True
)
```

### DSPy Progressive Optimization (Library-First)

```python
import dspy
from dspy.teleprompt import MIPROv2, BootstrapFewShot
from llama_index.core import VectorStoreIndex

# Phase 1: Zero-Shot Optimization (No Training Data)
class DocMindRAG(dspy.Module):
    def __init__(self, index):
        super().__init__()
        self.index = index
        self.generate_answer = dspy.ChainOfThought(
            "context, question -> answer"
        )
    
    def forward(self, question):
        # Use existing LlamaIndex retriever
        retriever = self.index.as_retriever(similarity_top_k=5)
        nodes = retriever.retrieve(question)
        context = "\n".join([n.text for n in nodes])
        
        pred = self.generate_answer(context=context, question=question)
        return pred

# Configure DSPy with local LLM
dspy.settings.configure(
    lm=dspy.LM(model="openai/gpt-4", api_base="http://localhost:11434/v1")
)

# Zero-shot optimization - NO EXAMPLES NEEDED!
rag = DocMindRAG(index)
zero_shot_optimizer = MIPROv2(
    metric=lambda x, y, trace: len(y.answer) > 20,  # Simple quality metric
    auto="light",  # Fast optimization
    num_threads=4
)

# Optimize with empty training set
optimized_rag = zero_shot_optimizer.compile(
    rag,
    trainset=[],  # No examples required!
    max_bootstrapped_demos=0,
    max_labeled_demos=0
)

# Phase 2: Few-Shot with Minimal Data (5-10 examples)
few_shot_examples = [
    dspy.Example(
        question="What is BGE-M3?",
        answer="BGE-M3 is a unified embedding model..."
    ).with_inputs("question"),
    # ... 4-9 more examples
]

few_shot_optimizer = BootstrapFewShot(
    metric=dspy.evaluate.answer_exact_match,
    max_bootstrapped_demos=4,  # Only 4 examples!
    max_labeled_demos=4
)

few_shot_rag = few_shot_optimizer.compile(
    rag,
    trainset=few_shot_examples[:5],
    valset=few_shot_examples[5:]
)

# Phase 3: Production Optimization (Collect real usage data)
# Implement after accumulating 20+ real user queries

# A/B Testing Framework
class DSPyABTest:
    def __init__(self, baseline_rag, optimized_rag):
        self.baseline = baseline_rag
        self.optimized = optimized_rag
        self.metrics = {"baseline": [], "optimized": []}
    
    def run_comparison(self, test_queries):
        for query in test_queries:
            baseline_result = self.baseline(question=query)
            optimized_result = self.optimized(question=query)
            
            # Calculate quality metrics
            self.metrics["baseline"].append({
                "query": query,
                "answer": baseline_result.answer,
                "latency": baseline_result.latency
            })
            self.metrics["optimized"].append({
                "query": query,
                "answer": optimized_result.answer,
                "latency": optimized_result.latency
            })
```

### vLLM FP8 Configuration (Native Support)

```python
from vllm import LLM, SamplingParams
import os

# Configure FP8 environment for RTX 4090 (Ada Lovelace)
os.environ["VLLM_ATTENTION_BACKEND"] = "FLASHINFER"

# Simple FP8 configuration - RTX 4090 supports FP8!
llm = LLM(
    model="Qwen/Qwen3-4B-Instruct-2507-FP8",  # Official FP8 model
    quantization="fp8",  # Native FP8 quantization
    kv_cache_dtype="fp8_e5m2",  # FP8 KV cache (50% memory reduction)
    gpu_memory_utilization=0.85,
    max_model_len=131072,  # 128K context possible with FP8
    trust_remote_code=True,
    enforce_eager=False,  # Enable CUDA graphs
    enable_chunked_prefill=True
)

# Alternative: Dynamic FP8 quantization of existing models
# llm = LLM(
#     model="Qwen/Qwen3-4B-Instruct-2507",
#     quantization="fp8",  # Dynamic quantization
#     kv_cache_dtype="fp8"
# )

# Integration with LlamaIndex
from llama_index.llms.vllm import Vllm

vllm_llm = Vllm(
    model="Qwen/Qwen3-4B-Instruct-2507-FP8",
    tensor_parallel_size=1,
    dtype="float16",
    quantization="fp8",
    kv_cache_dtype="fp8_e5m2",
    max_new_tokens=2048
)

# Set as default LLM
from llama_index.core import Settings
Settings.llm = vllm_llm

# Performance validation
import torch

def validate_fp8_performance():
    """Validate FP8 performance on RTX 4090."""
    vram_gb = torch.cuda.memory_allocated() / 1024**3
    print(f"VRAM usage with FP8: {vram_gb:.2f}GB")
    
    # Expected performance on RTX 4090:
    # - Decode: 120-180 tokens/second
    # - Prefill: 900-1400 tokens/second  
    # - Memory: 50% reduction vs FP16
    # - Context: 2x longer contexts possible
    
    assert vram_gb < 14, f"Total VRAM {vram_gb:.2f}GB exceeds 14GB limit"
    print("âœ… FP8 optimization validated on RTX 4090")
```

## 5. Data Contracts

### Multimodal Document Schema

```json
{
  "id": "doc_uuid",
  "content": {
    "text": "Document text content",
    "images": [
      {
        "id": "img_001",
        "path": "images/diagram1.png",
        "caption": "System architecture diagram",
        "embedding": [0.1, 0.2, ...],  // 512-dim CLIP
        "metadata": {
          "width": 1920,
          "height": 1080,
          "format": "PNG"
        }
      }
    ]
  },
  "embeddings": {
    "bge_m3_dense": [0.1, 0.2, ...],  // 1024-dim
    "bge_m3_sparse": {12: 0.5, ...},
    "bge_m3_colbert": [[0.1, ...], ...],  // Token-level
    "clip_text": [0.1, 0.2, ...],  // 512-dim cross-modal
    "clip_image_avg": [0.1, 0.2, ...]  // Average of image embeddings
  },
  "graph": {
    "entities": [
      {"text": "DocMind AI", "type": "PRODUCT", "confidence": 0.95},
      {"text": "RTX 4090", "type": "TECH", "confidence": 0.98}
    ],
    "relationships": [
      {
        "source": "DocMind AI",
        "target": "RTX 4090",
        "type": "OPTIMIZED_FOR",
        "confidence": 0.87
      }
    ]
  }
}
```

### PropertyGraph Schema

```json
{
  "graph_metadata": {
    "total_entities": 1567,
    "total_relationships": 3421,
    "entity_types": ["PERSON", "ORG", "PRODUCT", "TECH", "LOCATION"],
    "relationship_types": ["USES", "MENTIONS", "RELATED_TO", "PART_OF"],
    "last_updated": "2025-01-21T10:00:00Z"
  },
  "entity": {
    "id": "entity_uuid",
    "text": "LlamaIndex",
    "type": "TECH",
    "properties": {
      "category": "framework",
      "version": "0.12.0",
      "language": "Python"
    },
    "document_refs": ["doc_001", "doc_045"],
    "confidence": 0.92
  },
  "relationship": {
    "id": "rel_uuid",
    "source": "entity_uuid_1",
    "target": "entity_uuid_2",
    "type": "USES",
    "properties": {
      "context": "for retrieval pipeline",
      "frequency": 15
    },
    "confidence": 0.85
  }
}
```

### DSPy Optimization Schema

```json
{
  "optimization_session": {
    "id": "opt_session_uuid",
    "original_query": "How does BGE-M3 compare to BGE-large?",
    "optimization_strategy": "chain_of_thought",
    "variants": [
      {
        "query": "BGE-M3 vs BGE-large performance comparison",
        "score": 0.92,
        "strategy": "comparison",
        "reasoning": "Focused comparison query"
      },
      {
        "query": "What are the differences between BGE-M3 and BGE-large embeddings?",
        "score": 0.88,
        "strategy": "analytical",
        "reasoning": "Detailed differences query"
      },
      {
        "query": "BGE-M3 improvements over BGE-large context window memory",
        "score": 0.85,
        "strategy": "specific",
        "reasoning": "Technical specifics query"
      }
    ],
    "performance": {
      "optimization_latency_ms": 145,
      "quality_improvement": 0.24,  // 24% improvement
      "selected_variant": 0,
      "retrieval_scores": {
        "original": 0.71,
        "optimized": 0.92
      }
    }
  }
}
```

## 6. Implementation Instructions

### Phase 1: CLIP Integration (2 days)

#### Step 1: Configure ClipEmbedding

```bash
# Install ClipEmbedding dependencies (if not already installed)
uv add llama-index-embeddings-clip
```

#### Step 2: Update Embedding Configuration

1. Configure `ClipEmbedding` in settings with batch size optimization
2. Test VRAM usage validation (<1.4GB)
3. Update Qdrant collections to support CLIP vectors (512-dim)
4. Validate cross-modal embedding generation

#### Step 3: Integrate with Existing Infrastructure

1. Add CLIP to existing `MultiModalVectorStoreIndex`
2. Update RouterQueryEngine for image query detection
3. Test image document ingestion pipeline
4. Validate performance metrics

### Phase 2: PropertyGraphIndex Configuration (3 days)

#### Step 1: Configure Existing PropertyGraphIndex

```bash
# No new files needed - configure existing LlamaIndex features
# Update existing property_graph.py configuration
```

#### Step 2: Tune Domain-Specific Extractors

1. Configure `SimpleLLMPathExtractor` for technical documentation
2. Set up `SchemaLLMPathExtractor` with tech entity types
3. Tune extraction parameters for performance
4. Test with sample documents

#### Step 3: Integration and Testing

1. Integrate with existing Qdrant vector store
2. Test multi-hop graph traversal (depth=2)
3. Validate hybrid vector+graph retrieval
4. Performance testing with realistic document sets

### Phase 3: DSPy Progressive Implementation (2 weeks)

#### Week 1: Zero-Shot Optimization

1. Implement `DocMindRAG` class with LlamaIndex integration
2. Configure `MIPROv2` optimizer for zero-shot optimization
3. Test with synthetic queries (no training data required)
4. Validate basic quality improvements

#### Week 2: Few-Shot and A/B Testing

1. Collect 5-10 real example queries
2. Implement `BootstrapFewShot` optimization
3. Create A/B testing framework
4. Validate >20% quality improvement
5. Set up metrics collection for production optimization

### Phase 4: vLLM FP8 Setup (1 day)

#### Step 1: Configure FP8 vLLM

```bash
# Install FP8-capable vLLM (if not already installed)
uv add "vllm[flashinfer]>=0.10.1"

# Download pre-quantized FP8 model
huggingface-cli download Qwen/Qwen3-4B-Instruct-2507-FP8
```

#### Step 2: Update LLM Configuration

1. Configure vLLM with `quantization="fp8"` and `kv_cache_dtype="fp8_e5m2"`
2. Set FlashInfer backend via environment variable
3. Update LlamaIndex Settings to use FP8 vLLM
4. Validate performance: 120-180 tokens/sec decode, <14GB VRAM

### Phase 5: Integration Testing and Validation (1 week)

#### Step 1: End-to-End Testing

1. Test complete multimodal + graph + DSPy pipeline
2. Validate all performance targets
3. Test VRAM constraints with all models loaded
4. Benchmark against baseline system

#### Step 2: Documentation and Coverage

1. Update configuration documentation
2. Add integration tests for new configurations
3. Achieve 40%+ test coverage target
4. Performance regression testing

## 7. Change Plan

### Modified Files (Library-First Approach)

```text
# Configuration changes only - no new complex implementations
src/config/settings.py                    # Add CLIP, PropertyGraph, DSPy, FP8 config
src/retrieval/integration.py              # Wire ClipEmbedding and PropertyGraphIndex
src/retrieval/query_engine/router_engine.py # Add multimodal query detection
src/retrieval/graph/property_graph.py     # Configure extractors, not replace
src/retrieval/optimization/dspy_optimizer.py # Progressive DSPy implementation
src/core/infrastructure/vllm_config.py    # FP8 configuration
pyproject.toml                            # Add llama-index-embeddings-clip

# New minimal configuration files
src/retrieval/embeddings/clip_config.py   # ClipEmbedding configuration
src/retrieval/optimization/dspy_progressive.py # Progressive DSPy optimization
src/utils/multimodal.py                   # Multimodal utility functions

# Test files for new configurations  
tests/test_retrieval/test_clip_integration.py
tests/test_retrieval/test_property_graph_config.py
tests/test_retrieval/test_dspy_progressive.py
tests/test_retrieval/test_fp8_integration.py
```

### Dependencies Added

```toml
# Minimal additional dependencies
llama-index-embeddings-clip = ">=0.3.0"  # For ClipEmbedding
dspy-ai = ">=2.4.0"                       # For progressive optimization
vllm = {version = ">=0.10.1", extras = ["flashinfer"]}  # For FP8 support
```

### No Files Deleted

- PropertyGraphIndex is already functional - just needs configuration
- DSPy optimizer gets enhanced, not replaced
- All existing functionality preserved

## 8. Acceptance Criteria

### Scenario 1: Multimodal Image Search

```gherkin
Given a document corpus with text and architecture diagrams
When a user uploads an image of a system diagram
Then CLIP generates a 512-dimensional embedding within 100ms
And the system retrieves similar diagrams and related documentation
And image embeddings use less than 1.4GB VRAM on RTX 4090
And cross-modal search returns relevant text documents
```

### Scenario 2: PropertyGraph Relationship Query

```gherkin
Given a query "How are LlamaIndex and BGE-M3 connected?"
When RouterQueryEngine detects a relationship query
Then PropertyGraphIndex is automatically selected
And entities "LlamaIndex" and "BGE-M3" are extracted
And graph traversal finds connection paths up to 2 hops
And results include entity relationships and confidence scores
And graph query completes within 3 seconds
```

### Scenario 3: DSPy Query Optimization

```gherkin
Given DSPy optimization is enabled with 20 bootstrap examples
When a complex analytical query is submitted
Then DSPy generates 3-5 optimized query variants
And each variant has a different search strategy
And the best variant improves retrieval NDCG@10 by >20%
And optimization adds less than 200ms latency
And A/B testing validates the improvement
```

### Scenario 4: vLLM FP8 Performance

```gherkin
Given vLLM with FP8 KV cache on RTX 4090 Laptop
When processing a 128K context query
Then decode throughput exceeds 120 tokens/second
And prefill throughput exceeds 900 tokens/second
And memory usage stays under 14GB total
And FP8 quantization reduces KV cache by 50%
```

### Scenario 5: End-to-End Multimodal + Graph

```gherkin
Given a technical documentation corpus with diagrams
When a user asks "Show me systems similar to this architecture"
And provides an architecture diagram image
Then CLIP embedding is generated for the image
And similar diagrams are retrieved via vector search
And PropertyGraphIndex identifies related system entities
And results combine visual similarity and entity relationships
And the complete pipeline executes in under 5 seconds
```

## 9. Tests

### Unit Tests (New)

- `test_clip_embedding_generation` - Validate CLIP embeddings
- `test_clip_vram_constraint` - Ensure <1.4GB VRAM usage
- `test_entity_extraction` - Validate spaCy NER
- `test_graph_construction` - Test graph building
- `test_dspy_query_variants` - Validate query generation
- `test_vllm_fp8_config` - Test FP8 settings

### Integration Tests (New)

- `test_multimodal_retrieval_pipeline` - End-to-end image search
- `test_graph_vector_hybrid` - Combined retrieval strategies
- `test_dspy_improvement_validation` - Quality metrics
- `test_router_multimodal_routing` - Query classification

### Performance Tests (New)

- `test_clip_latency` - <100ms per image
- `test_graph_traversal_speed` - <3s for 2-hop
- `test_dspy_optimization_overhead` - <200ms
- `test_vllm_throughput` - >120 tokens/sec decode

### Coverage Requirements

- New modules: 70%+ coverage
- Modified modules: Maintain existing coverage
- Overall retrieval: Achieve 40%+ coverage

## 10. Security Considerations

- **Image Upload**: Validate file types and sizes
- **Graph Traversal**: Limit depth to prevent DoS
- **DSPy Prompts**: Sanitize generated queries
- **Local-Only**: No external API calls for embeddings
- **VRAM Monitoring**: Prevent OOM attacks

## 11. Quality Gates

### Performance Gates

- CLIP embedding generation: <100ms per image
- Graph query: <3s for 2-hop traversal
- DSPy optimization: <200ms overhead
- vLLM decode: >120 tokens/second
- End-to-end multimodal: <5 seconds

### Resource Gates

- CLIP model: <1.4GB VRAM (strict)
- PropertyGraphIndex: <500MB RAM
- DSPy cache: <200MB
- Total VRAM with all models: <14GB

### Quality Gates

- DSPy improvement: >20% NDCG@10 increase
- Graph entity extraction: >85% precision
- CLIP similarity: >0.8 cosine similarity for duplicates
- Test coverage: >40% for retrieval modules

## 12. Requirements Covered

This delta specification completes:

- **REQ-0044**: âœ… CLIP ViT-B/32 image embeddings (0% â†’ 100%)
- **REQ-0049**: âœ… PropertyGraphIndex implementation (25% â†’ 100%)
- **REQ-0050**: âœ… DSPy optimization integration (25% â†’ 100%)

Additional ADR compliance:

- **ADR-002**: Full ColBERT integration in BGE-M3
- **ADR-010**: Real vLLM FP8 optimization
- **ADR-018**: Complete DSPy with bootstrap
- **ADR-019**: Full PropertyGraphIndex with LlamaIndex

## 13. Dependencies

### New Python Dependencies

```toml
# Multimodal
transformers = ">=4.36.0"  # For CLIP
Pillow = ">=10.0.0"  # Image processing
torchvision = ">=0.16.0"  # Image transforms

# Graph
spacy = ">=3.7.0"  # Entity extraction
en-core-web-sm = ">=3.7.0"  # spaCy English model

# DSPy
dspy-ai = ">=2.4.0"  # Query optimization

# vLLM
vllm = {version = ">=0.10.1", extras = ["flashinfer"]}
```

### Model Dependencies

- `openai/clip-vit-base-patch32` (~605MB)
- `en_core_web_sm` (~12MB)
- `Qwen/Qwen3-4B-Instruct-2507-FP8` (already required)

## 14. Traceability

### Parent Documents

- **FEAT-002**: 002-retrieval-search.spec.md (parent specification)
- **ADR-002**: Unified Embedding Strategy (ColBERT support)
- **ADR-010**: Performance Optimization (FP8/vLLM)
- **ADR-018**: DSPy Prompt Optimization (full implementation)
- **ADR-019**: Optional GraphRAG (PropertyGraphIndex)

### Validation Criteria

- All FEAT-002 requirements achieve 100% completion
- All ADR requirements fully implemented (no stubs)
- Test coverage reaches 40%+ target
- Performance gates met on RTX 4090 Laptop

## 15. Timeline

### Implementation Schedule (3-4 weeks)

- **Week 1**: CLIP integration (2 days) + PropertyGraphIndex configuration (3 days)
- **Week 2**: DSPy zero-shot optimization + few-shot implementation
- **Week 3**: vLLM FP8 setup (1 day) + integration testing + validation
- **Week 4**: Performance optimization + documentation + coverage

### Milestones

- **Day 2**: CLIP multimodal working with <1.4GB VRAM
- **Day 5**: PropertyGraphIndex configured for technical docs
- **Week 2**: DSPy progressive optimization validated
- **Week 3**: FP8 performance targets met
- **Week 4**: All requirements at 100% with library-first approach

## 16. Success Metrics

### Completion Metrics

- REQ-0044: LlamaIndex ClipEmbedding configured with <1.4GB VRAM
- REQ-0049: PropertyGraphIndex configured with domain extractors (not stubbed)
- REQ-0050: DSPy progressive optimization achieving >20% quality improvement
- Test Coverage: >40% for all retrieval modules

### Performance Metrics

- P95 latency: <2 seconds (maintained)
- Multimodal search: <5 seconds end-to-end
- Graph traversal: <3 seconds for 2-hop
- DSPy optimization: >20% NDCG improvement
- FP8 acceleration: 120-180 tokens/sec decode

### Quality Metrics

- Library-first implementation: 70% less custom code
- All existing functionality preserved and enhanced
- Full ADR compliance with native library features
- Production-ready configuration with proper validation

## 17. Risk Mitigation

### Technical Risks

- **VRAM Constraints**: Implement dynamic model loading/unloading
- **Graph Size**: Use pagination and depth limits
- **DSPy Latency**: Cache optimized queries
- **Test Coverage**: Incremental testing during development

### Mitigation Strategies

- Feature flags for gradual rollout
- Comprehensive error handling
- Performance monitoring from day 1
- Regular checkpoint reviews

---

**Document Status**: This delta specification provides the complete implementation plan to achieve 100% FEAT-002 completion, transforming all stubs into production-ready implementations while maintaining the high quality established in the initial FEAT-002 implementation.
