{
  "atomic_changes": [
    {
      "acceptance_criteria": [
        "pyproject.toml updated with dependency changes",
        "uv.lock regenerated successfully",
        "No broken imports in codebase",
        "Package count reduced from 331 to ~310",
        "Installation time improved by 10%+"
      ],
      "changes": [
        {
          "action": "remove_dependencies",
          "commands": [
            "uv remove torchvision polars ragatouille"
          ],
          "impact": "7.5MB+ package size reduction, 21 fewer packages",
          "targets": [
            "torchvision==0.22.1",
            "polars",
            "ragatouille"
          ]
        },
        {
          "action": "add_explicit_dependency",
          "commands": [
            "uv add \"psutil>=6.0.0\""
          ],
          "reason": "Currently transitive but directly used in src/utils/monitoring.py",
          "target": "psutil>=6.0.0"
        },
        {
          "action": "move_to_dev_dependencies",
          "commands": [
            "uv add --group dev arize-phoenix openinference-instrumentation-llama-index",
            "uv remove arize-phoenix openinference-instrumentation-llama-index"
          ],
          "reason": "Development/optional observability tools only",
          "targets": [
            "arize-phoenix",
            "openinference-instrumentation-llama-index"
          ]
        }
      ],
      "description": "Remove unused dependencies and clean up package tree",
      "pr_id": "PR1",
      "priority": "P0",
      "risk_level": "low",
      "rollback_procedure": [
        "git checkout HEAD~1 -- pyproject.toml uv.lock",
        "uv sync --frozen"
      ],
      "size": "small",
      "title": "Critical Dependency Cleanup",
      "verification_commands": [
        "uv tree | grep -E \"(torchvision|polars|ragatouille)\" # Should return no matches",
        "python -c \"import sys; assert 'torchvision' not in sys.modules; print('✓ torchvision removed')\"",
        "python -c \"import psutil; print('✓ psutil available')\"",
        "uv pip check # No dependency conflicts"
      ]
    },
    {
      "acceptance_criteria": [
        "llama-cpp-python compiled with CUDA support",
        "PyTorch detects RTX 4090 GPU",
        "CMAKE_ARGS properly configured for compute capability 8.9",
        "No compilation errors during installation",
        "GPU acceleration functional"
      ],
      "changes": [
        {
          "action": "install_cuda_optimized_llama_cpp",
          "commands": [
            "CMAKE_ARGS=\"-DGGML_CUDA=on -DCUDA_ARCHITECTURES=89\" uv add \"llama-cpp-python[cuda]>=0.2.32,<0.3.0\""
          ],
          "impact": "Full GPU acceleration with RTX 4090 compute capability 8.9"
        },
        {
          "action": "install_pytorch_cuda",
          "commands": [
            "uv add torch==2.7.1 --index-url https://download.pytorch.org/whl/cu128"
          ],
          "impact": "CUDA 12.8 optimized PyTorch with FlexAttention"
        },
        {
          "action": "update_supporting_libraries",
          "commands": [
            "uv add tiktoken==0.9.0 numba==0.61.2"
          ],
          "impact": "Latest optimizations for tokenization and JIT compilation"
        },
        {
          "action": "create_cuda_environment_template",
          "content": [
            "# RTX 4090 CUDA Optimization",
            "export CUDA_VISIBLE_DEVICES=0",
            "export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True",
            "export TORCH_CUDNN_V8_API_ENABLED=1",
            "export CMAKE_ARGS=\"-DGGML_CUDA=on -DCUDA_ARCHITECTURES=89\""
          ],
          "file": ".env.cuda"
        }
      ],
      "description": "Install CUDA-optimized builds for maximum RTX 4090 utilization",
      "pr_id": "PR2",
      "priority": "P0",
      "risk_level": "medium",
      "rollback_procedure": [
        "uv remove llama-cpp-python torch",
        "uv add \"llama-cpp-python>=0.2.32,<0.3.0\" torch==2.7.1"
      ],
      "size": "medium",
      "title": "CUDA-Optimized Library Installation",
      "verification_commands": [
        "python -c \"import torch; assert torch.cuda.is_available(); print(f'✓ CUDA: {torch.version.cuda}')\"",
        "python -c \"import torch; print(f'✓ GPU: {torch.cuda.get_device_name(0)}')\"",
        "python -c \"from llama_cpp import Llama; print('✓ llama-cpp-python CUDA build')\"",
        "nvidia-smi # Should show GPU accessible"
      ]
    },
    {
      "acceptance_criteria": [
        "GPU configuration module created",
        "Environment variables properly set",
        "KV cache optimization implemented",
        "Memory management utilities functional",
        "Configuration tested with sample model"
      ],
      "changes": [
        {
          "action": "create_gpu_config_module",
          "features": [
            "RTX 4090 GPU detection and configuration",
            "Environment variable setup for CUDA optimization",
            "KV cache configuration (int8/int4 options)",
            "Memory management utilities",
            "llama-cpp-python optimization settings"
          ],
          "file": "src/core/gpu_config.py"
        },
        {
          "action": "implement_kv_cache_optimization",
          "strategies": {
            "default": {
              "memory_savings": "50%",
              "quality_impact": "<1% perplexity increase",
              "type": "int8"
            },
            "memory_pressure": {
              "memory_savings": "75%",
              "quality_impact": "1-2% perplexity increase",
              "type": "int4"
            }
          }
        },
        {
          "action": "configure_pytorch_optimizations",
          "optimizations": [
            "torch.backends.cuda.enable_flash_sdp(True)",
            "torch.backends.cuda.matmul.allow_tf32 = True",
            "PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
          ]
        }
      ],
      "description": "Implement GPU configuration and KV cache optimization",
      "pr_id": "PR3",
      "priority": "P1",
      "risk_level": "low",
      "size": "small",
      "title": "Runtime GPU Configuration",
      "verification_commands": [
        "python -c \"from src.core.gpu_config import GPUConfig; cfg=GPUConfig(); cfg.setup_environment(); print('✓ GPU config loaded')\"",
        "python -c \"import torch; print('✓ Flash SDP:', torch.backends.cuda.flash_sdp_enabled())\"",
        "python -c \"import torch; print('✓ TF32:', torch.backends.cuda.matmul.allow_tf32)\""
      ]
    },
    {
      "acceptance_criteria": [
        "GPU monitoring utilities implemented",
        "Thermal throttling detection functional",
        "VRAM usage tracking accurate",
        "Performance baselines established",
        "Monitoring integrated with existing logging"
      ],
      "changes": [
        {
          "action": "create_gpu_monitor",
          "features": [
            "Real-time GPU utilization tracking",
            "VRAM usage monitoring",
            "Temperature monitoring and thermal throttling detection",
            "Power draw tracking",
            "Performance metrics history"
          ],
          "file": "src/monitoring/gpu_monitor.py"
        },
        {
          "action": "implement_thermal_protection",
          "thresholds": {
            "power_limit": "450W (RTX 4090 TDP)",
            "thermal_warning": "85°C",
            "vram_warning": "14GB (87.5% of 16GB)"
          }
        },
        {
          "action": "integrate_with_logging",
          "alerts": [
            "GPU temperature warnings",
            "VRAM pressure alerts",
            "Performance degradation notifications"
          ],
          "target": "loguru logging framework"
        }
      ],
      "description": "Add GPU monitoring and thermal protection",
      "pr_id": "PR4",
      "priority": "P2",
      "risk_level": "low",
      "size": "medium",
      "title": "Performance Monitoring & Thermal Management",
      "verification_commands": [
        "python -c \"from src.monitoring.gpu_monitor import RTX4090Monitor; m=RTX4090Monitor(); print('✓ Monitoring initialized')\"",
        "nvidia-smi --query-gpu=temperature.gpu,memory.used --format=csv",
        "python -c \"import psutil; print('✓ CPU info:', psutil.cpu_count())\""
      ]
    },
    {
      "acceptance_criteria": [
        "All integration tests pass",
        "GPU optimization guide created",
        "Performance baselines documented",
        "Troubleshooting procedures validated",
        "Zero regressions in existing functionality"
      ],
      "changes": [
        {
          "action": "create_gpu_integration_tests",
          "file": "tests/integration/test_gpu_optimization.py",
          "test_categories": [
            "CUDA availability and RTX 4090 detection",
            "GPU configuration setup validation",
            "llama-cpp-python GPU compilation verification",
            "Performance monitoring functionality",
            "GPU matrix operation benchmarking"
          ]
        },
        {
          "action": "create_optimization_guide",
          "file": "docs/gpu-optimization-guide.md",
          "sections": [
            "Quick start guide for RTX 4090",
            "Performance tuning recommendations",
            "KV cache and quantization strategies",
            "Monitoring and troubleshooting procedures"
          ]
        },
        {
          "action": "create_performance_benchmarks",
          "benchmarks": {
            "installation_time": {
              "target": "10%+ improvement after torchvision removal",
              "test": "Full dependency installation"
            },
            "matrix_multiplication": {
              "target": "<5ms per operation on RTX 4090",
              "test": "4096x4096 FP16 matrix operations"
            },
            "memory_efficiency": {
              "target": "Support 70B Q4_K_M with 32K context",
              "test": "Load largest possible model in 16GB VRAM"
            }
          }
        }
      ],
      "description": "Comprehensive testing and documentation",
      "pr_id": "PR5",
      "priority": "P2",
      "risk_level": "low",
      "size": "medium",
      "title": "Integration Testing & Documentation",
      "verification_commands": [
        "pytest tests/integration/test_gpu_optimization.py -v",
        "python -c \"from src.core.gpu_config import GPUConfig; print('✓ All modules importable')\"",
        "nvidia-smi --query-gpu=name --format=csv,noheader # Should show RTX 4090"
      ]
    }
  ],
  "dependency_integration": {
    "applied_policies": {
      "cuda_optimization": {
        "llama_cpp_python": "CMAKE_ARGS=\"-DGGML_CUDA=on -DCUDA_ARCHITECTURES=89\" uv add llama-cpp-python[cuda]",
        "target_compute_capability": "8.9",
        "torch": "uv add torch==2.7.1 --index-url https://download.pytorch.org/whl/cu128"
      },
      "dependency_cleanup": {
        "add_explicit": [
          "psutil>=6.0.0"
        ],
        "move_to_dev": [
          "arize-phoenix",
          "openinference-instrumentation-llama-index"
        ],
        "remove": [
          "polars",
          "ragatouille",
          "torchvision"
        ],
        "total_package_reduction": "331 → ~310 packages"
      },
      "remove_torchvision": {
        "command": "uv remove torchvision",
        "confidence": "88.75%",
        "impact": "Faster installation, reduced complexity",
        "reason": "Completely unused, 7.5MB+ package savings"
      }
    },
    "source_files": [
      "library_research/10-llm_runtime_core-research.md",
      "library_research/10-llm_runtime_core-plan.json",
      "library_research/01-dependency_actions.json"
    ]
  },
  "integration_plan": {
    "created_date": "2025-08-12",
    "integrator": "@lib-integration-llm_runtime_core",
    "plan_name": "llm_runtime_core_integration",
    "target_hardware": {
      "cpu": "32-thread Intel i9-14900HX",
      "cuda_compute_capability": "8.9",
      "gpu": "NVIDIA RTX 4090 Laptop",
      "os": "Windows 11 + WSL2 (Ubuntu)",
      "ram": "64GB",
      "vram": "16GB GDDR6"
    },
    "version": "2.0"
  },
  "next_actions": {
    "immediate": [
      "Execute PR1: Critical Dependency Cleanup",
      "Validate torchvision removal and package count reduction",
      "Prepare CUDA environment for optimized library installation"
    ],
    "priority_sequence": [
      "PR1: Dependency cleanup (immediate cost savings)",
      "PR2: CUDA optimization (core GPU acceleration)",
      "PR3: Runtime configuration (performance optimization)",
      "PR4: Monitoring (operational safety)",
      "PR5: Testing and documentation (integration validation)"
    ]
  },
  "performance_targets": {
    "installation_improvements": {
      "installation_time_improvement": "10%+ faster uv sync",
      "package_count_reduction": "331 → ~310 packages",
      "package_size_reduction": "7.5MB+ (torchvision removal)"
    },
    "monitoring_targets": {
      "performance_degradation_detection": "25%+ slowdown triggers investigation",
      "temperature_threshold": "85°C warning, 90°C emergency",
      "vram_threshold": "14GB warning (87.5% utilization)"
    },
    "runtime_performance": {
      "context_support": "32K tokens with 16GB VRAM",
      "gpu_utilization": ">90% efficiency on RTX 4090",
      "matrix_operations": "<5ms for 4096x4096 FP16 on RTX 4090",
      "memory_optimization": "50%+ memory savings with KV cache"
    }
  },
  "risk_mitigation": {
    "high_risk_scenarios": [
      {
        "detection": "torch.cuda.is_available() == False",
        "impact": "high",
        "mitigation": "Revert to CPU-only builds, maintain fallback configuration",
        "probability": "medium",
        "rollback_commands": [
          "unset CMAKE_ARGS",
          "uv remove llama-cpp-python",
          "uv add \"llama-cpp-python>=0.2.32,<0.3.0\""
        ],
        "scenario": "CUDA installation failure"
      },
      {
        "detection": "CUDA out of memory errors",
        "fallback_sequence": [
          "Enable int8 KV cache (50% memory savings)",
          "Enable int4 KV cache (75% memory savings)",
          "Reduce context window from 32K to 16K",
          "Use Q4_K_S quantization instead of Q4_K_M"
        ],
        "impact": "high",
        "mitigation": "Progressive quantization fallback: int8 → int4 → Q4_K_S",
        "probability": "medium",
        "scenario": "VRAM exhaustion"
      }
    ],
    "medium_risk_scenarios": [
      {
        "automatic_actions": [
          "Log thermal warning",
          "Reduce GPU layer count by 25%",
          "Lower power limit to 80% if available"
        ],
        "detection": "GPU temperature >85°C",
        "impact": "medium",
        "mitigation": "Power limiting, improved cooling, reduced GPU layers",
        "probability": "medium",
        "scenario": "Thermal throttling"
      },
      {
        "detection": "nvidia-smi fails or GPU not detected",
        "impact": "medium",
        "mitigation": "Comprehensive documentation, fallback to CPU mode",
        "probability": "low",
        "scenario": "WSL2 GPU driver complexity"
      }
    ],
    "universal_rollback": {
      "commands": [
        "git checkout HEAD~N -- pyproject.toml uv.lock # N = number of commits",
        "uv sync --frozen",
        "unset CUDA_VISIBLE_DEVICES PYTORCH_CUDA_ALLOC_CONF CMAKE_ARGS"
      ],
      "validation": [
        "python -c \"import torch; print('PyTorch imported successfully')\"",
        "python -c \"from src.core import *; print('Core modules importable')\"",
        "uv pip check # Verify no dependency conflicts"
      ]
    }
  },
  "success_metrics": {
    "qualitative_targets": [
      "Comprehensive monitoring and thermal protection active",
      "Zero regressions in existing LLM functionality",
      "All integration tests passing with GPU acceleration",
      "Clear documentation and troubleshooting guides",
      "Maintainable, library-first implementation approach"
    ],
    "quantitative_targets": [
      "7.5MB+ package size reduction from dependency cleanup",
      "RTX 4090 GPU fully utilized with >90% efficiency",
      "KV cache optimization providing 50%+ memory savings",
      "Matrix operations <5ms for 4096x4096 FP16",
      "Support for 32K token context in 16GB VRAM"
    ]
  },
  "timeline": {
    "week_1": {
      "day_1_2": "Remove unused dependencies and validate",
      "day_3_5": "Install CUDA-optimized libraries and validate GPU access",
      "deliverables": [
        "Torchvision removed with 7.5MB+ savings",
        "CUDA-optimized llama-cpp-python installed",
        "PyTorch with CUDA 12.8 support installed",
        "GPU detection and basic acceleration verified"
      ],
      "focus": "Core Dependencies (PR1-PR2)"
    },
    "week_2": {
      "day_1_3": "Implement GPU configuration and runtime optimization",
      "day_4_5": "Add performance monitoring and thermal management",
      "deliverables": [
        "GPU configuration module with KV cache optimization",
        "Thermal monitoring and VRAM tracking implemented",
        "Performance baselines established",
        "Runtime optimization settings configured"
      ],
      "focus": "Optimization & Monitoring (PR3-PR4)"
    },
    "week_3": {
      "day_1_3": "Comprehensive testing and documentation",
      "day_4_5": "Performance validation and final integration",
      "deliverables": [
        "Complete integration test suite",
        "GPU optimization guide and documentation",
        "Performance benchmarks validated",
        "Zero regressions confirmed"
      ],
      "focus": "Integration & Validation (PR5)"
    }
  },
  "validation_suite": {
    "cuda_validation": [
      {
        "command": "python -c \"import torch; assert torch.cuda.is_available(); print(f'GPU: {torch.cuda.get_device_name(0)}')\"",
        "expected": "RTX 4090 detected and accessible",
        "test": "cuda_detection_validation"
      },
      {
        "command": "python -c \"from llama_cpp import Llama; print('llama-cpp-python CUDA build successful')\"",
        "expected": "No compilation errors, CUDA support confirmed",
        "test": "llama_cpp_gpu_validation"
      }
    ],
    "dependency_validation": [
      {
        "command": "python -c \"import sys; assert 'torchvision' not in sys.modules\"",
        "expected": "No ImportError, clean import check",
        "test": "torchvision_removal_validation"
      },
      {
        "command": "python -c \"import psutil; print(f'psutil version: {psutil.__version__}')\"",
        "expected": "psutil>=6.0.0 available as explicit dependency",
        "test": "psutil_explicit_dependency"
      }
    ],
    "integration_validation": [
      {
        "command": "python -c \"from src.core.gpu_config import GPUConfig; print('✓ GPU config available')\"",
        "expected": "GPU configuration module importable",
        "test": "gpu_config_import"
      },
      {
        "command": "python -c \"from src.monitoring.gpu_monitor import RTX4090Monitor; print('✓ GPU monitoring available')\"",
        "expected": "GPU monitoring functionality available",
        "test": "monitoring_integration"
      }
    ],
    "performance_validation": [
      {
        "command": "nvidia-smi --query-gpu=memory.used --format=csv,noheader,nounits",
        "expected": "Baseline VRAM usage measurement <2GB idle",
        "test": "vram_utilization_test"
      },
      {
        "command": "pytest tests/integration/test_gpu_optimization.py::TestGPUOptimization::test_gpu_matrix_performance -v",
        "expected": "4096x4096 FP16 matrix multiplication <5ms average",
        "test": "gpu_matrix_benchmark"
      }
    ]
  }
}
