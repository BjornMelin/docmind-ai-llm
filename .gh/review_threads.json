{"data":{"repository":{"pullRequest":{"reviewThreads":{"nodes":[{"id":"PRRT_kwDONtruzM5ZrI17","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLou","author":{"login":"copilot-pull-request-reviewer"},"body":"[nitpick] The comment explains the change from PDF to TXT but should clarify that this is to avoid heavy I/O operations during unit testing while preserving the core chunking parameter validation behavior.\n```suggestion\n    # Use a TXT file instead of PDF to avoid heavy I/O operations (like PDF parsing/rendering)\n    # during unit testing, while still validating chunking parameter forwarding behavior.\n```","path":"tests/unit/processing/test_document_processor_unit.py","originalLine":143,"diffHunk":"@@ -140,7 +140,8 @@ async def test_property_chunking_param_forwarding(\n     # Ensure ordering: combine_under < new_after < max_chars\n     assume(combine_under < new_after < max_chars)\n \n-    test_file = tmp_path / \"doc.pdf\"\n+    # Use non-PDF to avoid triggering PDF image rendering during this unit test."}]}},{"id":"PRRT_kwDONtruzM5ZrI18","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLov","author":{"login":"copilot-pull-request-reviewer"},"body":"The `current_data or self.results` fallback may pass the wrong data type. If `current_data` is None, `self.results` (likely a list) is passed where a dict might be expected by the regression tracker.\n```suggestion\n                # Ensure current_data is a dict for the regression tracker\n                if current_data is not None:\n                    metric_data = current_data\n                elif isinstance(self.results, dict):\n                    metric_data = self.results\n                elif isinstance(self.results, list) and self.results:\n                    # Assume self.results is a list of dicts, take the latest\n                    metric_data = self.results[-1]\n                else:\n                    metric_data = {}\n                regression_check = self.regression_tracker.check_regression(\n                    metric,\n                    current_data=metric_data,\n```","path":"scripts/performance_monitor.py","originalLine":435,"diffHunk":"@@ -353,7 +430,11 @@ def check_performance_regressions(\n \n             # Check critical metrics\n             for metric in CRITICAL_METRICS:\n-                regression_check = self.regression_tracker.check_regression(metric)\n+                regression_check = self.regression_tracker.check_regression(\n+                    metric,\n+                    current_data=current_data or self.results,"}]}},{"id":"PRRT_kwDONtruzM5ZrI19","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLoy","author":{"login":"copilot-pull-request-reviewer"},"body":"The `run` method expects `documents` to be Document objects but applies transformations directly to them as if they were nodes. This type mismatch could cause errors if the transformations expect BaseNode objects instead of Document objects.\n```suggestion\n        def run(self, nodes=None, show_progress=False):\n            nodes = nodes or []\n```","path":"tests/unit/processing/test_deterministic_ids_unit.py","originalLine":43,"diffHunk":"@@ -0,0 +1,63 @@\n+\"\"\"Unit tests for deterministic IDs and lineage in processing.\"\"\"\n+\n+from __future__ import annotations\n+\n+from pathlib import Path\n+from unittest.mock import Mock, patch\n+\n+import pytest\n+\n+from src.processing.document_processor import DocumentProcessor\n+from src.processing.utils import sha256_id\n+\n+\n+@pytest.mark.unit\n+def test_sha256_id_determinism() -> None:\n+    \"\"\"sha256_id returns stable values for equivalent inputs.\"\"\"\n+    a = sha256_id(\"/path/to/file.pdf\", \"1\", \" Hello\\nWorld  \")\n+    b = sha256_id(\"/path/to/file.pdf\", \"1\", \"Hello World\")\n+    assert a == b\n+    assert len(a) == 64\n+\n+\n+@pytest.mark.unit\n+@pytest.mark.asyncio\n+async def test_element_node_id_and_parent_id_present(tmp_path: Path) -> None:\n+    \"\"\"Element nodes carry deterministic node_id and lineage to parent hash.\"\"\"\n+    f = tmp_path / \"doc.txt\"\n+    f.write_text(\"hello\")\n+\n+    # Unstructured partition returns one element with page_number\n+    elem = Mock()\n+    elem.text = \" test  text \"\n+    elem.category = \"NarrativeText\"\n+    elem.metadata = Mock()\n+    elem.metadata.page_number = 1\n+\n+    class _FakePipeline:\n+        def __init__(self, transformations=None, cache=None, docstore=None):\n+            self.transformations = transformations or []\n+            self.cache = type(\"C\", (), {\"hits\": 0, \"misses\": 0})()\n+\n+        def run(self, documents=None, show_progress=False):\n+            nodes = documents or []"}]}},{"id":"PRRT_kwDONtruzM5ZrI1_","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLo0","author":{"login":"copilot-pull-request-reviewer"},"body":"[nitpick] The `_normalize_text` function is included in `__all__` despite having a leading underscore, which conventionally indicates a private function. Consider either removing the underscore to make it public or excluding it from `__all__`.\n```suggestion\n__all__ = [\"is_unstructured_like\", \"sha256_id\"]\n```","path":"src/processing/utils.py","originalLine":76,"diffHunk":"@@ -32,3 +71,6 @@ def is_unstructured_like(element: Any) -> bool:\n     # Avoid passing unittest.mock-based metadata into real chunkers\n     mod = getattr(getattr(meta, \"__class__\", object), \"__module__\", \"\")\n     return not mod.startswith(\"unittest\")\n+\n+\n+__all__ = [\"_normalize_text\", \"is_unstructured_like\", \"sha256_id\"]"}]}},{"id":"PRRT_kwDONtruzM5ZrI2A","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLo1","author":{"login":"copilot-pull-request-reviewer"},"body":"[nitpick] The method iterates through potentially many report files but doesn't limit the search scope. Consider adding a limit to only check the most recent N files (e.g., 10) to avoid performance issues when there are many historical reports.","path":"tests/performance/performance_regression_tracker.py","originalLine":104,"diffHunk":"@@ -0,0 +1,156 @@\n+\"\"\"Minimal performance regression tracker used by scripts/performance_monitor.py.\n+\n+Persists a simple JSON baseline of key metrics and compares current values\n+against the baseline using a percentage threshold.\n+\n+Baseline path: tests/performance/baselines/baseline.json\n+Current data: provided by caller (preferred). If unavailable, attempts to use\n+the most recent report file in tests/performance/reports/ (optional).\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+from typing import Any\n+\n+BASELINE_DIR = Path(\"tests/performance/baselines\")\n+BASELINE_FILE = BASELINE_DIR / \"baseline.json\"\n+REPORTS_DIR = Path(\"tests/performance/reports\")\n+\n+\n+class RegressionTracker:\n+    \"\"\"Simple baseline store + regression checker.\"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"Initialize tracker.\"\"\"\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n+\n+    # --- Baseline IO ---\n+    def _load_baseline(self) -> dict[str, Any]:\n+        if BASELINE_FILE.exists():\n+            with BASELINE_FILE.open(\"r\", encoding=\"utf-8\") as f:\n+                return json.load(f)\n+        return {}\n+\n+    def _save_baseline(self, data: dict[str, Any]) -> None:\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        with BASELINE_FILE.open(\"w\", encoding=\"utf-8\") as f:\n+            json.dump(data, f, indent=2)\n+\n+    # --- Public API used by performance_monitor ---\n+    def record_performance(\n+        self,\n+        metric: str,\n+        value: float,\n+        unit: str,\n+        kind: str,\n+        metadata: dict[str, Any] | None = None,\n+    ) -> None:\n+        \"\"\"Update baseline with a single metric value.\n+\n+        Called by PerformanceMonitor.record_performance_baseline().\n+        \"\"\"\n+        baseline = self._load_baseline()\n+        baseline.setdefault(\"metrics\", {})\n+        baseline[\"metrics\"][metric] = {\n+            \"value\": float(value),\n+            \"unit\": unit,\n+            \"kind\": kind,\n+            \"metadata\": metadata or {},\n+        }\n+        self._save_baseline(baseline)\n+\n+    def _extract_current_value(\n+        self, metric: str, current_data: dict[str, Any] | None\n+    ) -> float | None:\n+        \"\"\"Best-effort extraction of current value for a known metric name.\"\"\"\n+        if not current_data:\n+            return None\n+        # Map our canonical metric names to PerformanceMonitor result keys\n+        mapping = {\n+            \"test_suite_duration\": [\"total_duration\", \"duration\", \"suite_duration\"],\n+            \"test_collection_time\": [\"collection_time\"],\n+            \"average_test_duration\": [\"average_duration\"],\n+            # The following are not currently emitted by the monitor;\n+            # keep for future use\n+            \"memory_usage_peak\": [\"memory_usage_peak\"],\n+            \"gpu_vram_peak_mb\": [\"gpu_vram_peak_mb\"],\n+            \"embedding_latency\": [\"embedding_latency\"],\n+            \"retrieval_latency\": [\"retrieval_latency\"],\n+            \"llm_inference_time\": [\"llm_inference_time\"],\n+        }\n+        keys = mapping.get(metric, [])\n+        for k in keys:\n+            if k in current_data:\n+                try:\n+                    return float(current_data[k])\n+                except (TypeError, ValueError):\n+                    return None\n+        return None\n+\n+    def _latest_report_value(self, metric: str) -> float | None:\n+        \"\"\"Fallback: inspect the latest saved report for the metric.\"\"\"\n+        reports = sorted(REPORTS_DIR.glob(\"performance_data_*.json\"), reverse=True)\n+        for p in reports:\n+            try:\n+                data = json.loads(p.read_text(encoding=\"utf-8\"))\n+                val = self._extract_current_value(metric, data)\n+                if val is not None:\n+                    return val\n+            except (OSError, ValueError, UnicodeDecodeError):\n+                continue\n+        return None"}]}},{"id":"PRRT_kwDONtruzM5ZrI3Y","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqY","author":{"login":"sourcery-ai"},"body":"**suggestion (bug_risk):** Suppressing exceptions when hashing image bytes may hide underlying issues.\n\nInstead of setting img_hash to an empty string on failure, log the exception or issue a warning to help identify potential file corruption or access problems.","path":"src/processing/document_processor.py","originalLine":598,"diffHunk":"@@ -573,6 +589,53 @@ async def process_document_async(\n             # Convert LlamaIndex nodes to DocumentElements for compatibility\n             processed_elements = self._convert_nodes_to_elements(nodes)\n \n+            # If PDF, emit page-image nodes for multimodal reranking\n+            if file_path.suffix.lower() == \".pdf\":\n+                # Lazy import to avoid importing PyMuPDF unless needed\n+                from src.processing.pdf_pages import save_pdf_page_images\n+\n+                images_dir = (\n+                    Path(getattr(self.settings, \"cache_dir\", \"./cache\"))"}]}},{"id":"PRRT_kwDONtruzM5ZrI3a","isResolved":true,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqa","author":{"login":"sourcery-ai"},"body":"**suggestion (bug_risk):** Idempotent image writing may not update images if the source PDF changes.\n\nSince images are only written if they don't exist, updates to the PDF won't be reflected in the output. Implement a check (such as comparing file hashes) to ensure images are refreshed when the PDF changes.","path":"src/processing/pdf_pages.py","originalLine":86,"diffHunk":"@@ -51,4 +52,54 @@ def pdf_pages_to_image_documents(\n     return docs, out_dir\n \n \n-__all__ = [\"pdf_pages_to_image_documents\"]\n+def save_pdf_page_images(pdf_path: Path, out_dir: Path, dpi: int = 180) -> list[dict]:\n+    \"\"\"Render each PDF page to a PNG with a stable filename and return metadata.\n+\n+    - Stable filename format: ``<stem>__page-<n>.png`` (1-based page numbering)\n+    - Returns one item per page with page number, image path, and page bbox\n+    - Idempotent: existing PNG files are not re-written\n+\n+    Args:\n+        pdf_path: Path to the source PDF\n+        out_dir: Directory to store generated images\n+        dpi: Render resolution (dots per inch)\n+\n+    Returns:\n+        A list of dicts: {\"page_no\": int, \"image_path\": str, \"bbox\": [x0,y0,x1,y1]}\n+    \"\"\"\n+    pdf_path = Path(pdf_path)\n+    out_dir = Path(out_dir)\n+    out_dir.mkdir(parents=True, exist_ok=True)\n+\n+    results: list[dict] = []\n+\n+    with fitz.open(pdf_path) as doc:\n+        zoom = dpi / 72.0\n+        mat = fitz.Matrix(zoom, zoom)\n+        for idx, page in enumerate(doc, start=1):\n+            # Stable filename, idempotent write behavior\n+            img_name = f\"{pdf_path.stem}__page-{idx}.png\"\n+            img_path = out_dir / img_name\n+\n+            if not img_path.exists():\n+                pix = page.get_pixmap(matrix=mat)\n+                pix.save(str(img_path))"}]}},{"id":"PRRT_kwDONtruzM5ZrI3b","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqb","author":{"login":"sourcery-ai"},"body":"**suggestion (testing):** PDF page image utility is tested for stable filenames and bbox.\n\nPlease add tests for multi-page PDFs and for cases where image files already exist to verify correct processing and idempotency.","path":"tests/unit/processing/test_pdf_pages_unit.py","originalLine":16,"diffHunk":"@@ -0,0 +1,55 @@\n+\"\"\"Unit tests for PDF page image emission utilities.\n+\n+Offline and deterministic: PyMuPDF is patched with a lightweight stub.\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+from pathlib import Path\n+from types import SimpleNamespace\n+from unittest.mock import patch\n+\n+import pytest\n+\n+\n+@pytest.mark.unit\n+def test_save_pdf_page_images_stable_names_and_bbox(tmp_path: Path) -> None:"}]}},{"id":"PRRT_kwDONtruzM5ZrI3d","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqd","author":{"login":"sourcery-ai"},"body":"**suggestion (testing):** LLM backend validation tests updated for strict set.\n\nPlease add a test case for unsupported backends to confirm that errors or warnings are raised as expected.","path":"tests/unit/config/test_settings.py","originalLine":462,"diffHunk":"@@ -457,16 +458,16 @@ class TestLLMBackendValidation:\n     \"\"\"Test LLM backend validation and warnings.\"\"\"\n \n     def test_valid_llm_backends(self):\n-        \"\"\"Test all valid LLM backends are accepted.\"\"\"\n-        valid_backends = [\"ollama\", \"llamacpp\", \"vllm\", \"openai\"]\n+        \"\"\"Test all valid LLM backends are accepted (strict set).\"\"\"\n+        valid_backends = [\"ollama\", \"llamacpp\", \"vllm\", \"lmstudio\"]"}]}},{"id":"PRRT_kwDONtruzM5ZrI3e","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqe","author":{"login":"sourcery-ai"},"body":"**issue (complexity):** Consider refactoring shared PDF rendering logic into a private helper function used by both public APIs to eliminate code duplication.\n\nYou can pull all of the “open‐pdf → render pages → save if missing → collect bbox” logic into one small helper, then have both public functions call it and just map its output to whatever shape they need:\n\n```python\n# new private helper\ndef _render_pdf_pages(\n    pdf_path: Path, out_dir: Path, dpi: int = 180\n) -> list[tuple[int, Path, fitz.Rect]]:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    zoom = dpi / 72.0\n    mat = fitz.Matrix(zoom, zoom)\n    results: list[tuple[int, Path, fitz.Rect]] = []\n    with fitz.open(pdf_path) as doc:\n        for i, page in enumerate(doc, start=1):\n            img_name = f\"{pdf_path.stem}__page-{i}.png\"\n            img_path = out_dir / img_name\n            if not img_path.exists():\n                page.get_pixmap(matrix=mat).save(str(img_path))\n            results.append((i, img_path, page.rect))\n    return results\n```\n\nThen simplify your two public functions to just call it:\n\n```python\ndef save_pdf_page_images(\n    pdf_path: Path, out_dir: Path, dpi: int = 180\n) -> list[dict]:\n    entries = _render_pdf_pages(pdf_path, out_dir, dpi)\n    return [\n        {\n            \"page_no\": i,\n            \"image_path\": str(path),\n            \"bbox\": [float(rect.x0), float(rect.y0),\n                     float(rect.x1), float(rect.y1)],\n        }\n        for i, path, rect in entries\n    ]\n\n\ndef pdf_pages_to_image_documents(\n    pdf_path: Path, dpi: int = 180\n) -> tuple[list[ImageDocument], Path]:\n    out_dir = Path(tempfile.mkdtemp(prefix=uuid.uuid4().hex))\n    entries = _render_pdf_pages(pdf_path, out_dir, dpi)\n    docs = [\n        ImageDocument(\n            image_path=str(path),\n            metadata={\n                \"page\": i,\n                \"modality\": \"pdf_page_image\",\n                \"source\": str(pdf_path),\n            },\n        )\n        for i, path, _ in entries\n    ]\n    return docs, out_dir\n```\n\nThis removes all duplication while preserving both APIs.","path":"src/processing/pdf_pages.py","originalLine":55,"diffHunk":"@@ -51,4 +52,54 @@ def pdf_pages_to_image_documents(\n     return docs, out_dir\n \n \n-__all__ = [\"pdf_pages_to_image_documents\"]\n+def save_pdf_page_images(pdf_path: Path, out_dir: Path, dpi: int = 180) -> list[dict]:"}]}},{"id":"PRRT_kwDONtruzM5ZrI3f","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqf","author":{"login":"sourcery-ai"},"body":"**suggestion (code-quality):** Swap if/else branches of if expression to remove negation ([`swap-if-expression`](https://docs.sourcery.ai/Reference/Rules-and-In-Line-Suggestions/Python/Default-Rules/swap-if-expression))\n\n```suggestion\n            \"llamacpp\": _OLike if getattr(settings, \"llamacpp_base_url\", None) else _LCpp,\n\n```\n\n<br/><details><summary>Explanation</summary>Negated conditions are more difficult to read than positive ones, so it is best\nto avoid them where we can. By swapping the `if` and `else` conditions around we\ncan invert the condition and make it positive.\n</details>","path":"tests/integration/test_settings_page.py","originalLine":124,"diffHunk":"@@ -113,6 +113,46 @@ def __init__(self, *_, **__):\n     monkeypatch.setitem(_sys.modules, \"llama_index.llms.ollama\", ollama_mod)\n     monkeypatch.setitem(_sys.modules, \"llama_index.llms.llama_cpp\", llama_cpp_mod)\n \n+    # Stub factory to avoid import complexity; return deterministic kind per backend\n+    def _mk_llm(settings) -> object:  # type: ignore[override]\n+        mapping = {\n+            \"ollama\": _Ollama,\n+            \"vllm\": _OLike,\n+            \"lmstudio\": _OLike,\n+            \"llamacpp\": _LCpp\n+            if not getattr(settings, \"llamacpp_base_url\", None)\n+            else _OLike,"}]}},{"id":"PRRT_kwDONtruzM5ZrI3g","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqg","author":{"login":"sourcery-ai"},"body":"**issue (code-quality):** We've found these issues:\n\n- Extract code out into method ([`extract-method`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/extract-method/))\n- Replace if statement with if expression ([`assign-if-exp`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/assign-if-exp/))","path":"scripts/performance_monitor.py","originalLine":113,"diffHunk":"@@ -98,6 +113,15 @@ def run_test_suite_performance(\n             logger.info(\"Starting test suite performance measurement...\")"}]}},{"id":"PRRT_kwDONtruzM5ZrI3i","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqi","author":{"login":"sourcery-ai"},"body":"**suggestion (code-quality):** We've found these issues:\n\n- Lift code into else after jump in control flow ([`reintroduce-else`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/reintroduce-else/))\n- Replace if statement with if expression ([`assign-if-exp`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/assign-if-exp/))\n\n```suggestion\n    return \"\" if not value else \" \".join(value.split()).strip()\n```","path":"src/processing/utils.py","originalLine":27,"diffHunk":"@@ -5,9 +5,48 @@\n \n from __future__ import annotations\n \n+import hashlib\n from typing import Any\n \n \n+def _normalize_text(value: str) -> str:\n+    \"\"\"Normalize text for stable hashing.\n+\n+    - Strips leading/trailing whitespace\n+    - Collapses internal whitespace to a single space\n+\n+    Args:\n+        value: Input text\n+\n+    Returns:\n+        Normalized text suitable for hashing\n+    \"\"\"\n+    if not value:\n+        return \"\"\n+    # Collapse runs of whitespace and strip\n+    return \" \".join(value.split()).strip()"}]}},{"id":"PRRT_kwDONtruzM5ZrI3j","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqj","author":{"login":"sourcery-ai"},"body":"**suggestion (code-quality):** Remove unnecessary casts to int, str, float or bool ([`remove-unnecessary-cast`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/remove-unnecessary-cast/))\n\n```suggestion\n            \"value\": value,\n```","path":"tests/performance/performance_regression_tracker.py","originalLine":58,"diffHunk":"@@ -0,0 +1,156 @@\n+\"\"\"Minimal performance regression tracker used by scripts/performance_monitor.py.\n+\n+Persists a simple JSON baseline of key metrics and compares current values\n+against the baseline using a percentage threshold.\n+\n+Baseline path: tests/performance/baselines/baseline.json\n+Current data: provided by caller (preferred). If unavailable, attempts to use\n+the most recent report file in tests/performance/reports/ (optional).\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+from typing import Any\n+\n+BASELINE_DIR = Path(\"tests/performance/baselines\")\n+BASELINE_FILE = BASELINE_DIR / \"baseline.json\"\n+REPORTS_DIR = Path(\"tests/performance/reports\")\n+\n+\n+class RegressionTracker:\n+    \"\"\"Simple baseline store + regression checker.\"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"Initialize tracker.\"\"\"\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n+\n+    # --- Baseline IO ---\n+    def _load_baseline(self) -> dict[str, Any]:\n+        if BASELINE_FILE.exists():\n+            with BASELINE_FILE.open(\"r\", encoding=\"utf-8\") as f:\n+                return json.load(f)\n+        return {}\n+\n+    def _save_baseline(self, data: dict[str, Any]) -> None:\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        with BASELINE_FILE.open(\"w\", encoding=\"utf-8\") as f:\n+            json.dump(data, f, indent=2)\n+\n+    # --- Public API used by performance_monitor ---\n+    def record_performance(\n+        self,\n+        metric: str,\n+        value: float,\n+        unit: str,\n+        kind: str,\n+        metadata: dict[str, Any] | None = None,\n+    ) -> None:\n+        \"\"\"Update baseline with a single metric value.\n+\n+        Called by PerformanceMonitor.record_performance_baseline().\n+        \"\"\"\n+        baseline = self._load_baseline()\n+        baseline.setdefault(\"metrics\", {})\n+        baseline[\"metrics\"][metric] = {\n+            \"value\": float(value),"}]}},{"id":"PRRT_kwDONtruzM5ZrI3k","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLqk","author":{"login":"sourcery-ai"},"body":"**suggestion (code-quality):** Remove redundant exceptions from an except clause ([`remove-redundant-exception`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/remove-redundant-exception/))\n\n```suggestion\n            except (OSError, ValueError):\n```","path":"tests/performance/performance_regression_tracker.py","originalLine":102,"diffHunk":"@@ -0,0 +1,156 @@\n+\"\"\"Minimal performance regression tracker used by scripts/performance_monitor.py.\n+\n+Persists a simple JSON baseline of key metrics and compares current values\n+against the baseline using a percentage threshold.\n+\n+Baseline path: tests/performance/baselines/baseline.json\n+Current data: provided by caller (preferred). If unavailable, attempts to use\n+the most recent report file in tests/performance/reports/ (optional).\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+from typing import Any\n+\n+BASELINE_DIR = Path(\"tests/performance/baselines\")\n+BASELINE_FILE = BASELINE_DIR / \"baseline.json\"\n+REPORTS_DIR = Path(\"tests/performance/reports\")\n+\n+\n+class RegressionTracker:\n+    \"\"\"Simple baseline store + regression checker.\"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"Initialize tracker.\"\"\"\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n+\n+    # --- Baseline IO ---\n+    def _load_baseline(self) -> dict[str, Any]:\n+        if BASELINE_FILE.exists():\n+            with BASELINE_FILE.open(\"r\", encoding=\"utf-8\") as f:\n+                return json.load(f)\n+        return {}\n+\n+    def _save_baseline(self, data: dict[str, Any]) -> None:\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        with BASELINE_FILE.open(\"w\", encoding=\"utf-8\") as f:\n+            json.dump(data, f, indent=2)\n+\n+    # --- Public API used by performance_monitor ---\n+    def record_performance(\n+        self,\n+        metric: str,\n+        value: float,\n+        unit: str,\n+        kind: str,\n+        metadata: dict[str, Any] | None = None,\n+    ) -> None:\n+        \"\"\"Update baseline with a single metric value.\n+\n+        Called by PerformanceMonitor.record_performance_baseline().\n+        \"\"\"\n+        baseline = self._load_baseline()\n+        baseline.setdefault(\"metrics\", {})\n+        baseline[\"metrics\"][metric] = {\n+            \"value\": float(value),\n+            \"unit\": unit,\n+            \"kind\": kind,\n+            \"metadata\": metadata or {},\n+        }\n+        self._save_baseline(baseline)\n+\n+    def _extract_current_value(\n+        self, metric: str, current_data: dict[str, Any] | None\n+    ) -> float | None:\n+        \"\"\"Best-effort extraction of current value for a known metric name.\"\"\"\n+        if not current_data:\n+            return None\n+        # Map our canonical metric names to PerformanceMonitor result keys\n+        mapping = {\n+            \"test_suite_duration\": [\"total_duration\", \"duration\", \"suite_duration\"],\n+            \"test_collection_time\": [\"collection_time\"],\n+            \"average_test_duration\": [\"average_duration\"],\n+            # The following are not currently emitted by the monitor;\n+            # keep for future use\n+            \"memory_usage_peak\": [\"memory_usage_peak\"],\n+            \"gpu_vram_peak_mb\": [\"gpu_vram_peak_mb\"],\n+            \"embedding_latency\": [\"embedding_latency\"],\n+            \"retrieval_latency\": [\"retrieval_latency\"],\n+            \"llm_inference_time\": [\"llm_inference_time\"],\n+        }\n+        keys = mapping.get(metric, [])\n+        for k in keys:\n+            if k in current_data:\n+                try:\n+                    return float(current_data[k])\n+                except (TypeError, ValueError):\n+                    return None\n+        return None\n+\n+    def _latest_report_value(self, metric: str) -> float | None:\n+        \"\"\"Fallback: inspect the latest saved report for the metric.\"\"\"\n+        reports = sorted(REPORTS_DIR.glob(\"performance_data_*.json\"), reverse=True)\n+        for p in reports:\n+            try:\n+                data = json.loads(p.read_text(encoding=\"utf-8\"))\n+                val = self._extract_current_value(metric, data)\n+                if val is not None:\n+                    return val\n+            except (OSError, ValueError, UnicodeDecodeError):"}]}},{"id":"PRRT_kwDONtruzM5ZrI3l","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrLql","author":{"login":"sourcery-ai"},"body":"**suggestion (code-quality):** Remove unnecessary casts to int, str, float or bool ([`remove-unnecessary-cast`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/remove-unnecessary-cast/))\n\n```suggestion\n            \"regression_detected\": regression,\n```","path":"tests/performance/performance_regression_tracker.py","originalLine":143,"diffHunk":"@@ -0,0 +1,156 @@\n+\"\"\"Minimal performance regression tracker used by scripts/performance_monitor.py.\n+\n+Persists a simple JSON baseline of key metrics and compares current values\n+against the baseline using a percentage threshold.\n+\n+Baseline path: tests/performance/baselines/baseline.json\n+Current data: provided by caller (preferred). If unavailable, attempts to use\n+the most recent report file in tests/performance/reports/ (optional).\n+\"\"\"\n+\n+from __future__ import annotations\n+\n+import json\n+from pathlib import Path\n+from typing import Any\n+\n+BASELINE_DIR = Path(\"tests/performance/baselines\")\n+BASELINE_FILE = BASELINE_DIR / \"baseline.json\"\n+REPORTS_DIR = Path(\"tests/performance/reports\")\n+\n+\n+class RegressionTracker:\n+    \"\"\"Simple baseline store + regression checker.\"\"\"\n+\n+    def __init__(self) -> None:\n+        \"\"\"Initialize tracker.\"\"\"\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        REPORTS_DIR.mkdir(parents=True, exist_ok=True)\n+\n+    # --- Baseline IO ---\n+    def _load_baseline(self) -> dict[str, Any]:\n+        if BASELINE_FILE.exists():\n+            with BASELINE_FILE.open(\"r\", encoding=\"utf-8\") as f:\n+                return json.load(f)\n+        return {}\n+\n+    def _save_baseline(self, data: dict[str, Any]) -> None:\n+        BASELINE_DIR.mkdir(parents=True, exist_ok=True)\n+        with BASELINE_FILE.open(\"w\", encoding=\"utf-8\") as f:\n+            json.dump(data, f, indent=2)\n+\n+    # --- Public API used by performance_monitor ---\n+    def record_performance(\n+        self,\n+        metric: str,\n+        value: float,\n+        unit: str,\n+        kind: str,\n+        metadata: dict[str, Any] | None = None,\n+    ) -> None:\n+        \"\"\"Update baseline with a single metric value.\n+\n+        Called by PerformanceMonitor.record_performance_baseline().\n+        \"\"\"\n+        baseline = self._load_baseline()\n+        baseline.setdefault(\"metrics\", {})\n+        baseline[\"metrics\"][metric] = {\n+            \"value\": float(value),\n+            \"unit\": unit,\n+            \"kind\": kind,\n+            \"metadata\": metadata or {},\n+        }\n+        self._save_baseline(baseline)\n+\n+    def _extract_current_value(\n+        self, metric: str, current_data: dict[str, Any] | None\n+    ) -> float | None:\n+        \"\"\"Best-effort extraction of current value for a known metric name.\"\"\"\n+        if not current_data:\n+            return None\n+        # Map our canonical metric names to PerformanceMonitor result keys\n+        mapping = {\n+            \"test_suite_duration\": [\"total_duration\", \"duration\", \"suite_duration\"],\n+            \"test_collection_time\": [\"collection_time\"],\n+            \"average_test_duration\": [\"average_duration\"],\n+            # The following are not currently emitted by the monitor;\n+            # keep for future use\n+            \"memory_usage_peak\": [\"memory_usage_peak\"],\n+            \"gpu_vram_peak_mb\": [\"gpu_vram_peak_mb\"],\n+            \"embedding_latency\": [\"embedding_latency\"],\n+            \"retrieval_latency\": [\"retrieval_latency\"],\n+            \"llm_inference_time\": [\"llm_inference_time\"],\n+        }\n+        keys = mapping.get(metric, [])\n+        for k in keys:\n+            if k in current_data:\n+                try:\n+                    return float(current_data[k])\n+                except (TypeError, ValueError):\n+                    return None\n+        return None\n+\n+    def _latest_report_value(self, metric: str) -> float | None:\n+        \"\"\"Fallback: inspect the latest saved report for the metric.\"\"\"\n+        reports = sorted(REPORTS_DIR.glob(\"performance_data_*.json\"), reverse=True)\n+        for p in reports:\n+            try:\n+                data = json.loads(p.read_text(encoding=\"utf-8\"))\n+                val = self._extract_current_value(metric, data)\n+                if val is not None:\n+                    return val\n+            except (OSError, ValueError, UnicodeDecodeError):\n+                continue\n+        return None\n+\n+    def check_regression(\n+        self,\n+        metric: str,\n+        *,\n+        current_data: dict[str, Any] | None = None,\n+        threshold_pct: float = 20.0,\n+    ) -> dict[str, Any]:\n+        \"\"\"Compare current metric value against baseline.\n+\n+        Returns a dictionary with:\n+          - regression_detected (bool)\n+          - baseline_value, current_value, regression_factor, threshold_pct\n+          - reason (optional diagnostic)\n+        \"\"\"\n+        baseline = self._load_baseline()\n+        base_metrics = baseline.get(\"metrics\", {})\n+        base_entry = base_metrics.get(metric)\n+        if not base_entry:\n+            return {\"regression_detected\": False, \"reason\": \"no_baseline\"}\n+\n+        baseline_value = float(base_entry.get(\"value\", 0.0))\n+        if baseline_value <= 0:\n+            return {\"regression_detected\": False, \"reason\": \"invalid_baseline\"}\n+\n+        current_value = self._extract_current_value(metric, current_data)\n+        if current_value is None:\n+            current_value = self._latest_report_value(metric)\n+        if current_value is None:\n+            return {\"regression_detected\": False, \"reason\": \"no_current_data\"}\n+\n+        # Compute percentage increase (regression when larger-is-worse)\n+        delta = current_value - baseline_value\n+        pct_increase = (delta / baseline_value) * 100.0\n+        regression = pct_increase > threshold_pct\n+        factor = (current_value / baseline_value) if baseline_value else 1.0\n+\n+        return {\n+            \"regression_detected\": bool(regression),"}]}},{"id":"PRRT_kwDONtruzM5ZrJTM","isResolved":false,"comments":{"nodes":[{"id":"PRRC_kwDONtruzM6KrMK7","author":{"login":"chatgpt-codex-connector"},"body":"**[P1] Avoid ID collisions for blank-text elements**\n\nDeterministic IDs are derived from `file_path`, `page_no`, and `element.text`, but `element.text` is often empty for images and other non-text unstructured elements. When a PDF page contains multiple such elements, all nodes on that page get the same `det_id`/`doc_id`, so the ingestion `SimpleDocumentStore` overwrites earlier nodes and only one survives. For pages with more than one image or diagram this silently drops content. Include a value that is unique per element (e.g., `element_index` or the Unstructured `element_id`/coordinates) in the hash to keep IDs unique while remaining deterministic.\n\nUseful? React with 👍 / 👎.","path":"src/processing/document_processor.py","originalLine":337,"diffHunk":"@@ -324,6 +324,18 @@ def _safe_json_value(val: Any) -> Any:\n                     k: v for k, v in element_metadata.items() if v is not None\n                 }\n \n+            # Derive page number (if available) for deterministic IDs\n+            page_no = 0\n+            try:\n+                page_no = int(element_metadata.get(\"page_number\", 0))\n+            except Exception:\n+                page_no = 0\n+\n+            # Compute deterministic node id (source_path + page_no + normalized text)\n+            det_id = sha256_id(\n+                str(file_path), str(page_no), str(getattr(element, \"text\", \"\"))\n+            )"}]}}]}}}}}