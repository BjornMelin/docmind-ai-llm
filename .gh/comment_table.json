[
  {
    "id": 3261277082,
    "type": "issue",
    "author": "sourcery-ai[bot]",
    "path": null,
    "line": null,
    "body": "<!-- Generated by sourcery-ai[bot]: start review_guide -->\n\n## Reviewer's Guide\n\nImplements a standardized Unstructured‚ÜíLlamaIndex ingestion pipeline with configurable partition strategies (including OCR fallback), deterministic node IDs, PDF page image emission, and a local DuckDB-backed cache for idempotent, repeatable processing.\n\n#### Sequence diagram for ingestion pipeline with Unstructured, OCR fallback, and DuckDB cache\n\n```mermaid\nsequenceDiagram\n    participant User\n    participant \"DocumentProcessor\"\n    participant \"Unstructured(partition)\"\n    participant \"OCR Engine\"\n    participant \"LlamaIndex IngestionPipeline\"\n    participant \"DuckDBKVStore(IngestionCache)\"\n\n    User->>\"DocumentProcessor\": process_document_async(file_path)\n    \"DocumentProcessor\"->>\"DuckDBKVStore(IngestionCache)\": check cache for file_path\n    alt Cache hit\n        \"DuckDBKVStore(IngestionCache)\"-->>\"DocumentProcessor\": return cached result\n    else Cache miss\n        \"DocumentProcessor\"->>\"Unstructured(partition)\": partition(file_path, strategy)\n        alt Partition fails or scanned PDF\n            \"DocumentProcessor\"->>\"OCR Engine\": extract text via OCR\n            \"OCR Engine\"-->>\"DocumentProcessor\": OCR text\n        end\n        \"DocumentProcessor\"->>\"LlamaIndex IngestionPipeline\": ingest partitioned elements\n        \"LlamaIndex IngestionPipeline\"-->>\"DocumentProcessor\": nodes\n        \"DocumentProcessor\"->>\"DuckDBKVStore(IngestionCache)\": store result\n    end\n    \"DocumentProcessor\"-->>User: return processed elements\n```\n\n#### Sequence diagram for PDF page image emission during ingestion\n\n```mermaid\nsequenceDiagram\n    participant \"DocumentProcessor\"\n    participant \"save_pdf_page_images()\"\n    participant \"DuckDBKVStore(IngestionCache)\"\n\n    \"DocumentProcessor\"->>\"save_pdf_page_images()\": Render PDF pages to PNG\n    \"save_pdf_page_images()\"-->>\"DocumentProcessor\": Return image metadata (page_no, image_path, bbox)\n    \"DocumentProcessor\"->>\"DuckDBKVStore(IngestionCache)\": Store image node metadata\n    \"DocumentProcessor\"-->>\"DocumentProcessor\": Extend processed elements with image nodes\n```\n\n#### ER diagram for PDF page image node and document element\n\n```mermaid\nerDiagram\n    DOCUMENT_ELEMENT {\n        string doc_id\n        string text\n        string category\n        dict metadata\n    }\n    PDF_PAGE_IMAGE_NODE {\n        string node_id\n        int page_no\n        tuple bbox\n        string modality\n        string source_path\n        string hash\n    }\n    DOCUMENT_ELEMENT ||--o{ PDF_PAGE_IMAGE_NODE : contains\n```\n\n#### Class diagram for deterministic node IDs and PDF page image nodes\n\n```mermaid\nclassDiagram\n    class PdfPageImageNode {\n        +str node_id\n        +int page_no\n        +tuple bbox\n        +str modality\n        +str source_path\n        +str hash\n    }\n\n    class DocumentElement {\n        +str text\n        +str category\n        +dict metadata\n        +str doc_id\n        +list excluded_embed_metadata_keys\n    }\n\n    PdfPageImageNode <|-- DocumentElement : \"image node extends element\"\n```\n\n#### Flow diagram for ingestion pipeline with strategy mapping and caching\n\n```mermaid\nflowchart TD\n    A[\"Start: process_document_async(file_path)\"] --> B[\"Check DuckDBKVStore cache\"]\n    B -->|Cache hit| C[\"Return cached result\"]\n    B -->|Cache miss| D[\"Partition document with Unstructured (strategy mapping)\"]\n    D --> E[\"OCR fallback if needed\"]\n    E --> F[\"LlamaIndex IngestionPipeline: chunking & node creation\"]\n    F --> G[\"Emit PDF page images (if PDF)\"]\n    G --> H[\"Store results in DuckDBKVStore cache\"]\n    H --> I[\"Return processed elements\"]\n```\n\n### File-Level Changes\n\n| Change | Details | Files |\n| ------ | ------- | ----- |\n| Add Unstructured-based ingestion transform with strategy mapping and heuristic chunking | <ul><li>Introduce UnstructuredTransformation component orchestrating partition(auto) calls</li><li>Implement strategy mapping (hi_res, fast, ocr_only) with OCR fallback</li><li>Apply title-density heuristic to choose chunk_by_title vs basic chunk_elements</li><li>Wire transformation into LlamaIndex IngestionPipeline sequence</li></ul> | `src/processing/document_processor.py` |\n| Enable deterministic node IDs and parent lineage via SHA-256 hashing | <ul><li>Add _normalize_text and sha256_id utilities for stable hashing</li><li>Compute and assign node_id and parent_id in DocumentProcessor</li><li>Normalize text and metadata keys before hashing</li></ul> | `src/processing/utils.py`<br/>`src/processing/document_processor.py` |\n| Emit PDF page images with stable filenames and include bbox metadata | <ul><li>Implement save_pdf_page_images to render pages to __page-<n>.png idempotently</li><li>Append pdf_page_image DocumentElements with page_no, bbox, image_path</li><li>Define PdfPageImageNode schema in models for artifact metadata</li></ul> | `src/processing/pdf_pages.py`<br/>`src/processing/document_processor.py`<br/>`src/models/schemas.py` |\n| Integrate DuckDB-backed IngestionCache for idempotent runs and cache statistics | <ul><li>Initialize IngestionCache with DuckDBKVStore at ./cache/docmind.duckdb</li><li>Persist cache file creation and directory setup</li><li>Capture hit/miss stats and include in ProcessingResult metadata</li></ul> | `src/processing/document_processor.py` |\n| Expand test coverage and update scripts/docs for new ingestion and performance features | <ul><li>Add unit/integration tests for chunking heuristics, deterministic IDs, PDF images, and cache behavior</li><li>Enhance performance_monitor to collect CPU/GPU memory usage and integrate in docs/scripts</li><li>Standardize command invocations (uv run) across README, docs, and scripts</li></ul> | `tests/unit/processing/test_deterministic_ids_unit.py`<br/>`tests/unit/processing/test_pdf_pages_unit.py`<br/>`tests/integration/test_ingestion_pipeline_pdf_images.py`<br/>`scripts/performance_monitor.py`<br/>`README.md`<br/>`docs/` |\n\n---\n\n<details>\n<summary>Tips and commands</summary>\n\n#### Interacting with Sourcery\n\n- **Trigger a new review:** Comment `@sourcery-ai review` on the pull request.\n- **Continue discussions:** Reply directly to Sourcery's review comments.\n- **Generate a GitHub issue from a review comment:** Ask Sourcery to create an\n  issue from a review comment by replying to it. You can also reply to a\n  review comment with `@sourcery-ai issue` to create an issue from it.\n- **Generate a pull request title:** Write `@sourcery-ai` anywhere in the pull\n  request title to generate a title at any time. You can also comment\n  `@sourcery-ai title` on the pull request to (re-)generate the title at any time.\n- **Generate a pull request summary:** Write `@sourcery-ai summary` anywhere in\n  the pull request body to generate a PR summary at any time exactly where you\n  want it. You can also comment `@sourcery-ai summary` on the pull request to\n  (re-)generate the summary at any time.\n- **Generate reviewer's guide:** Comment `@sourcery-ai guide` on the pull\n  request to (re-)generate the reviewer's guide at any time.\n- **Resolve all Sourcery comments:** Comment `@sourcery-ai resolve` on the\n  pull request to resolve all Sourcery comments. Useful if you've already\n  addressed all the comments and don't want to see them anymore.\n- **Dismiss all Sourcery reviews:** Comment `@sourcery-ai dismiss` on the pull\n  request to dismiss all existing Sourcery reviews. Especially useful if you\n  want to start fresh with a new review - don't forget to comment\n  `@sourcery-ai review` to trigger a new review!\n\n#### Customizing Your Experience\n\nAccess your [dashboard](https://app.sourcery.ai) to:\n- Enable or disable review features such as the Sourcery-generated pull request\n  summary, the reviewer's guide, and others.\n- Change the review language.\n- Add, remove or edit custom review instructions.\n- Adjust other review settings.\n\n#### Getting Help\n\n- [Contact our support team](mailto:support@sourcery.ai) for questions or feedback.\n- Visit our [documentation](https://docs.sourcery.ai) for detailed guides and information.\n- Keep in touch with the Sourcery team by following us on [X/Twitter](https://x.com/SourceryAI), [LinkedIn](https://www.linkedin.com/company/sourcery-ai/) or [GitHub](https://github.com/sourcery-ai).\n\n</details>\n\n<!-- Generated by sourcery-ai[bot]: end review_guide -->"
  },
  {
    "id": 2326575662,
    "type": "review",
    "author": "Copilot",
    "path": "tests/unit/processing/test_document_processor_unit.py",
    "line": 143,
    "body": "[nitpick] The comment explains the change from PDF to TXT but should clarify that this is to avoid heavy I/O operations during unit testing while preserving the core chunking parameter validation behavior.\n```suggestion\n    # Use a TXT file instead of PDF to avoid heavy I/O operations (like PDF parsing/rendering)\n    # during unit testing, while still validating chunking parameter forwarding behavior.\n```"
  },
  {
    "id": 2326575663,
    "type": "review",
    "author": "Copilot",
    "path": "scripts/performance_monitor.py",
    "line": 435,
    "body": "The `current_data or self.results` fallback may pass the wrong data type. If `current_data` is None, `self.results` (likely a list) is passed where a dict might be expected by the regression tracker.\n```suggestion\n                # Ensure current_data is a dict for the regression tracker\n                if current_data is not None:\n                    metric_data = current_data\n                elif isinstance(self.results, dict):\n                    metric_data = self.results\n                elif isinstance(self.results, list) and self.results:\n                    # Assume self.results is a list of dicts, take the latest\n                    metric_data = self.results[-1]\n                else:\n                    metric_data = {}\n                regression_check = self.regression_tracker.check_regression(\n                    metric,\n                    current_data=metric_data,\n```"
  },
  {
    "id": 2326575666,
    "type": "review",
    "author": "Copilot",
    "path": "tests/unit/processing/test_deterministic_ids_unit.py",
    "line": 43,
    "body": "The `run` method expects `documents` to be Document objects but applies transformations directly to them as if they were nodes. This type mismatch could cause errors if the transformations expect BaseNode objects instead of Document objects.\n```suggestion\n        def run(self, nodes=None, show_progress=False):\n            nodes = nodes or []\n```"
  },
  {
    "id": 2326575668,
    "type": "review",
    "author": "Copilot",
    "path": "src/processing/utils.py",
    "line": 76,
    "body": "[nitpick] The `_normalize_text` function is included in `__all__` despite having a leading underscore, which conventionally indicates a private function. Consider either removing the underscore to make it public or excluding it from `__all__`.\n```suggestion\n__all__ = [\"is_unstructured_like\", \"sha256_id\"]\n```"
  },
  {
    "id": 2326575669,
    "type": "review",
    "author": "Copilot",
    "path": "tests/performance/performance_regression_tracker.py",
    "line": 104,
    "body": "[nitpick] The method iterates through potentially many report files but doesn't limit the search scope. Consider adding a limit to only check the most recent N files (e.g., 10) to avoid performance issues when there are many historical reports."
  },
  {
    "id": 2326575768,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "src/processing/document_processor.py",
    "line": 598,
    "body": "**suggestion (bug_risk):** Suppressing exceptions when hashing image bytes may hide underlying issues.\n\nInstead of setting img_hash to an empty string on failure, log the exception or issue a warning to help identify potential file corruption or access problems."
  },
  {
    "id": 2326575770,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "src/processing/pdf_pages.py",
    "line": 86,
    "body": "**suggestion (bug_risk):** Idempotent image writing may not update images if the source PDF changes.\n\nSince images are only written if they don't exist, updates to the PDF won't be reflected in the output. Implement a check (such as comparing file hashes) to ensure images are refreshed when the PDF changes."
  },
  {
    "id": 2326575771,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "tests/unit/processing/test_pdf_pages_unit.py",
    "line": 16,
    "body": "**suggestion (testing):** PDF page image utility is tested for stable filenames and bbox.\n\nPlease add tests for multi-page PDFs and for cases where image files already exist to verify correct processing and idempotency."
  },
  {
    "id": 2326575773,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "tests/unit/config/test_settings.py",
    "line": 462,
    "body": "**suggestion (testing):** LLM backend validation tests updated for strict set.\n\nPlease add a test case for unsupported backends to confirm that errors or warnings are raised as expected."
  },
  {
    "id": 2326575774,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "src/processing/pdf_pages.py",
    "line": 55,
    "body": "**issue (complexity):** Consider refactoring shared PDF rendering logic into a private helper function used by both public APIs to eliminate code duplication.\n\nYou can pull all of the ‚Äúopen‚Äêpdf ‚Üí render pages ‚Üí save if missing ‚Üí collect bbox‚Äù logic into one small helper, then have both public functions call it and just map its output to whatever shape they need:\n\n```python\n# new private helper\ndef _render_pdf_pages(\n    pdf_path: Path, out_dir: Path, dpi: int = 180\n) -> list[tuple[int, Path, fitz.Rect]]:\n    out_dir.mkdir(parents=True, exist_ok=True)\n    zoom = dpi / 72.0\n    mat = fitz.Matrix(zoom, zoom)\n    results: list[tuple[int, Path, fitz.Rect]] = []\n    with fitz.open(pdf_path) as doc:\n        for i, page in enumerate(doc, start=1):\n            img_name = f\"{pdf_path.stem}__page-{i}.png\"\n            img_path = out_dir / img_name\n            if not img_path.exists():\n                page.get_pixmap(matrix=mat).save(str(img_path))\n            results.append((i, img_path, page.rect))\n    return results\n```\n\nThen simplify your two public functions to just call it:\n\n```python\ndef save_pdf_page_images(\n    pdf_path: Path, out_dir: Path, dpi: int = 180\n) -> list[dict]:\n    entries = _render_pdf_pages(pdf_path, out_dir, dpi)\n    return [\n        {\n            \"page_no\": i,\n            \"image_path\": str(path),\n            \"bbox\": [float(rect.x0), float(rect.y0),\n                     float(rect.x1), float(rect.y1)],\n        }\n        for i, path, rect in entries\n    ]\n\n\ndef pdf_pages_to_image_documents(\n    pdf_path: Path, dpi: int = 180\n) -> tuple[list[ImageDocument], Path]:\n    out_dir = Path(tempfile.mkdtemp(prefix=uuid.uuid4().hex))\n    entries = _render_pdf_pages(pdf_path, out_dir, dpi)\n    docs = [\n        ImageDocument(\n            image_path=str(path),\n            metadata={\n                \"page\": i,\n                \"modality\": \"pdf_page_image\",\n                \"source\": str(pdf_path),\n            },\n        )\n        for i, path, _ in entries\n    ]\n    return docs, out_dir\n```\n\nThis removes all duplication while preserving both APIs."
  },
  {
    "id": 2326575775,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "tests/integration/test_settings_page.py",
    "line": 124,
    "body": "**suggestion (code-quality):** Swap if/else branches of if expression to remove negation ([`swap-if-expression`](https://docs.sourcery.ai/Reference/Rules-and-In-Line-Suggestions/Python/Default-Rules/swap-if-expression))\n\n```suggestion\n            \"llamacpp\": _OLike if getattr(settings, \"llamacpp_base_url\", None) else _LCpp,\n\n```\n\n<br/><details><summary>Explanation</summary>Negated conditions are more difficult to read than positive ones, so it is best\nto avoid them where we can. By swapping the `if` and `else` conditions around we\ncan invert the condition and make it positive.\n</details>"
  },
  {
    "id": 2326575776,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "scripts/performance_monitor.py",
    "line": 113,
    "body": "**issue (code-quality):** We've found these issues:\n\n- Extract code out into method ([`extract-method`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/extract-method/))\n- Replace if statement with if expression ([`assign-if-exp`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/assign-if-exp/))"
  },
  {
    "id": 2326575778,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "src/processing/utils.py",
    "line": 27,
    "body": "**suggestion (code-quality):** We've found these issues:\n\n- Lift code into else after jump in control flow ([`reintroduce-else`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/reintroduce-else/))\n- Replace if statement with if expression ([`assign-if-exp`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/assign-if-exp/))\n\n```suggestion\n    return \"\" if not value else \" \".join(value.split()).strip()\n```"
  },
  {
    "id": 2326575779,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "tests/performance/performance_regression_tracker.py",
    "line": 58,
    "body": "**suggestion (code-quality):** Remove unnecessary casts to int, str, float or bool ([`remove-unnecessary-cast`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/remove-unnecessary-cast/))\n\n```suggestion\n            \"value\": value,\n```"
  },
  {
    "id": 2326575780,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "tests/performance/performance_regression_tracker.py",
    "line": 102,
    "body": "**suggestion (code-quality):** Remove redundant exceptions from an except clause ([`remove-redundant-exception`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/remove-redundant-exception/))\n\n```suggestion\n            except (OSError, ValueError):\n```"
  },
  {
    "id": 2326575781,
    "type": "review",
    "author": "sourcery-ai[bot]",
    "path": "tests/performance/performance_regression_tracker.py",
    "line": 143,
    "body": "**suggestion (code-quality):** Remove unnecessary casts to int, str, float or bool ([`remove-unnecessary-cast`](https://docs.sourcery.ai/Reference/Default-Rules/refactorings/remove-unnecessary-cast/))\n\n```suggestion\n            \"regression_detected\": regression,\n```"
  },
  {
    "id": 2326577851,
    "type": "review",
    "author": "chatgpt-codex-connector[bot]",
    "path": "src/processing/document_processor.py",
    "line": 337,
    "body": "**[P1] Avoid ID collisions for blank-text elements**\n\nDeterministic IDs are derived from `file_path`, `page_no`, and `element.text`, but `element.text` is often empty for images and other non-text unstructured elements. When a PDF page contains multiple such elements, all nodes on that page get the same `det_id`/`doc_id`, so the ingestion `SimpleDocumentStore` overwrites earlier nodes and only one survives. For pages with more than one image or diagram this silently drops content. Include a value that is unique per element (e.g., `element_index` or the Unstructured `element_id`/coordinates) in the hash to keep IDs unique while remaining deterministic.\n\nUseful? React with üëç¬†/ üëé."
  }
]
