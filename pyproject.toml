[project]
name = "docmind_ai_llm"
version = "0.1.0"
description = "Local LLM for AI-Powered Document Analysis"
authors = [{ name = "Bjorn Melin" }]
license-files = ["LICENSE"]
readme = "README.md"
requires-python = ">=3.10,<3.13"
keywords = ["ai", "rag", "document-analysis", "llm", "hybrid-search"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Software Development :: Libraries :: Python Modules",
]
dependencies = [
    # Core application framework
    "streamlit>=1.48.0",
    "python-dotenv==1.1.1",
    "loguru>=0.7.3",
    "tenacity>=9.1.2",
    "diskcache==5.6.3",
    # Document processing
    "pymupdf==1.26.3",
    "python-docx==1.2.0",
    "pillow>=11.0.0,<12.0.0",
    "unstructured[all-docs]>=0.18.11",
    "pyarrow>=21.0.0,<22.0.0",
    # AI/ML core
    "transformers==4.55.4",
    "torch==2.7.1",
    "tiktoken==0.11.0",
    "openai>=1.98.0,<2.0.0",
    "openai-whisper==20250625",
    "spacy==3.8.7",
    # LLM backends
    "ollama==0.5.3",
    "llama-cpp-python>=0.2.32,<0.3.0", # CPU version only, GPU version in [gpu] extra
    # Vector database
    "qdrant-client==1.15.1",
    # LlamaIndex ecosystem - core
    "llama-index-core>=0.12.14,<0.13.0",
    "llama-index-vector-stores-qdrant",
    # LlamaIndex ecosystem - LLMs
    "llama-index-llms-openai",
    "llama-index-llms-ollama",
    "llama-index-llms-llama-cpp",
    "llama-index-llms-vllm>=0.5.1",
    # LlamaIndex ecosystem - embeddings
    "llama-index-embeddings-openai",
    "llama-index-embeddings-huggingface",
    "llama-index-embeddings-jinaai",
    "llama-index-embeddings-fastembed",
    "llama-index-embeddings-clip>=0.1.0",
    # LlamaIndex ecosystem - tools
    "llama-index-postprocessor-colbert-rerank",
    "llama-index-agent-openai",
    "llama-index-program-openai",
    "llama-index-readers-file",
    "llama-index-multi-modal-llms-openai",
    "llama-index-question-gen-openai",
    # Agent framework - LangGraph for multi-agent orchestration
    "langgraph>=0.6.6",
    "langgraph-supervisor>=0.0.29",
    "langchain-core>=0.3.74",
    "langchain-openai>=0.3.31",
    # BGE-M3 and retrieval dependencies for FEAT-002
    "FlagEmbedding>=1.3.5", # BGE-M3 unified embeddings
    "sentence-transformers>=5.1.0", # CrossEncoder reranking
    # DSPy for prompt optimization
    "dspy-ai>=2.5.0",
    "pydantic==2.11.7",
    "pydantic-settings==2.10.1",
    "vllm[flashinfer]>=0.10.1.1",
]

[project.optional-dependencies]
gpu = [
    "fastembed-gpu>=0.7.0",               # GPU-accelerated embeddings
    "vllm[flashinfer]>=0.10.1.1",         # vLLM with FlashInfer support
    # Build tools for GPU libraries
    "ninja>=1.11.1",                      # Build acceleration
    "cmake>=3.26.4",                      # Required for CUDA builds
]

# Note: spaCy models are not PyPI packages and must be installed separately:
# uv run python -m spacy download en_core_web_sm

[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project.urls]
Repository = "https://github.com/BjornMelin/docmind-ai"
Documentation = "https://github.com/BjornMelin/docmind-ai-llm/tree/main/docs"
"GPU Setup Guide" = "https://github.com/BjornMelin/docmind-ai-llm/blob/main/docs/developers/gpu-setup.md"

[project.scripts]
# Performance validation and GPU testing
gpu-validate = "scripts.gpu_validation:main"
performance-test = "scripts.performance_validation:main"

[tool.docmind-ai]
# CUDA Stack Requirements
cuda_version = "12.8+"
pytorch_version = "2.7.1"
vllm_version = ">=0.10.1.1"
driver_version = "550.54.14+"

# Hardware Requirements for Qwen3-4B-Instruct-2507-FP8
min_vram_gb = 16
recommended_vram_gb = 16
target_model = "Qwen3-4B-Instruct-2507-FP8"
max_context_length = 131072

# Performance Targets
target_decode_tps = [100, 160]  # tokens per second
target_prefill_tps = [800, 1300]  # tokens per second

# Installation Commands (documented in README.md)
installation_notes = """
Automated GPU Installation:
uv sync --extra gpu --index-strategy=unsafe-best-match

Manual Installation Steps (if needed):
1. Install PyTorch with CUDA 12.8:
   uv pip install torch==2.7.1 torchvision==0.22.1 torchaudio==2.7.1 --extra-index-url https://download.pytorch.org/whl/cu128

2. Install vLLM with FlashInfer:
   uv pip install "vllm[flashinfer]>=0.10.1.1" --extra-index-url https://download.pytorch.org/whl/cu128

3. Install llama-cpp-python with CUDA:
   uv pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu128

4. Install remaining GPU dependencies:
   uv pip install fastembed-gpu>=0.7.0
"""

[tool.ruff]
line-length = 88
target-version = "py312"

[tool.ruff.lint]
select = ["E", "F", "I", "UP", "N", "S", "B", "A", "C4", "PT", "SIM", "TID", "D"]
ignore = [
    "D203",  # 1 blank line required before class docstring (conflicts with D211)
    "D213",  # Multi-line docstring summary should start at the second line (conflicts with Google style)
    "S301",  # pickle usage (acceptable for local session persistence)
    "S603",  # subprocess call (acceptable for hardware detection)
    "S607",  # partial executable path (acceptable for well-known system commands)
    "S108",  # hardcoded temp file path (will be replaced with tempfile)
]

[tool.ruff.lint.per-file-ignores]
"tests/**/*.py" = [
    "S101",  # assert usage (standard practice in pytest tests)
    "S311",  # pseudo-random generators (acceptable for test timing simulation)
]

[tool.ruff.lint.isort]
known-first-party = ["docmind_ai"]

[tool.ruff.lint.pydocstyle]
convention = "google"


[tool.pytest.ini_options]
testpaths = ["tests"]
pythonpath = ["src"]
python_files = ["test_*.py"]
python_functions = ["test_*"]
addopts = [
    "--strict-markers",
    "--strict-config",
    "--tb=short",
    "-ra",
    "--import-mode=importlib",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "performance: marks tests as performance tests",
    "requires_gpu: marks tests that require GPU",
    "requires_network: marks tests that require network access",
]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"

[dependency-groups]
dev = [
    "ruff==0.12.8",
    "pytest>=8.3.1",
    "pytest-cov>=6.2.1",
    "pytest-asyncio>=1.1.0",
    "pytest-benchmark>=5.0.0",
    "hypothesis>=6.137.1",
]
test = [
    "pytest==8.3.1",
    "pytest-asyncio>=1.1.0",
    "pytest-cov>=6.2.1",
    "moviepy==2.2.1",  # Only needed for video processing tests
]
performance = [
    "numba>=0.61.0",  # Optional JIT compilation for performance
]
