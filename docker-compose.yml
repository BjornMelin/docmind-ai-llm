services:
  app:
    build: .
    ports:
      - "8501:8501"
    environment:
      - DOCMIND_LLM_BACKEND=ollama
      - DOCMIND_OLLAMA_BASE_URL=http://ollama:11434
      - DOCMIND_SECURITY__ENDPOINT_ALLOWLIST=http://ollama
      - DOCMIND_QDRANT__URL=http://qdrant:6333
      # GPU Configuration
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_TORCH_BACKEND=cu128
      - VLLM_ATTENTION_BACKEND=FLASHINFER
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
      - FASTEMBED_CACHE_PATH=/tmp/fastembed_cache
      - ONNXRUNTIME_PROVIDERS=CUDAExecutionProvider,CPUExecutionProvider
    volumes:
      - .:/app
      - ./models:/models
      - gpu_cache:/tmp/fastembed_cache
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      qdrant:
        condition: service_healthy
    networks:
      - internal
    runtime: nvidia
    ipc: host
    shm_size: '2gb'

  ollama:
    image: ollama/ollama:latest
    profiles: ["gpu"]
    volumes:
      - ollama:/root/.ollama
    networks:
      - internal
    runtime: nvidia

  qdrant:
    image: qdrant/qdrant:gpu-nvidia-latest
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__GPU__INDEXING=1
      - QDRANT__GPU__FORCE_HALF_PRECISION=true
      - QDRANT__GPU__GROUPS_COUNT=512
      - QDRANT__GPU__PARALLEL_INDEXES=1
      - QDRANT__GPU__DEVICE_FILTER=nvidia
      - QDRANT__GPU__ALLOW_INTEGRATED=false
    volumes:
      - qdrant_storage:/qdrant/storage
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:6333"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      - internal
    runtime: nvidia

networks:
  internal:
    internal: true

volumes:
  qdrant_storage:
  gpu_cache:
  ollama:
