# üß† DocMind AI: Local LLM for AI-Powered Document Analysis

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Streamlit](https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white)
![LangChain](https://img.shields.io/badge/ü¶ú_LangChain-2C2C2C?style=for-the-badge)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Ollama](https://img.shields.io/badge/ü¶ô_Ollama-000000?style=for-the-badge)

[![MIT License](https://img.shields.io/badge/License-MIT-green.svg)](https://choosealicense.com/licenses/mit/)
[![GitHub](https://img.shields.io/badge/GitHub-BjornMelin-181717?logo=github)](https://github.com/BjornMelin)
[![LinkedIn](https://img.shields.io/badge/LinkedIn-Bjorn%20Melin-0077B5?logo=linkedin)](https://www.linkedin.com/in/bjorn-melin/)

**DocMind AI** is a powerful, open-source Streamlit application that leverages the capabilities of Large Language Models (LLMs) running locally on your machine through [Ollama](https://ollama.com/). Analyze a vast array of document types, extract key insights, generate summaries, identify action items, and surface open questions ‚Äì all without sending your data to the cloud!

## ‚ú® Features of DocMind AI

- **Privacy-Focused:** Your documents are processed locally, ensuring data privacy and security.
- **Versatile Document Handling:** Supports a wide range of file formats:
  - üìÑ PDF
  - üìë DOCX
  - üìù TXT
  - üìä XLSX
  - üåê MD (Markdown)
  - üóÉÔ∏è JSON
  - üóÇÔ∏è XML
  - üî§ RTF
  - üìá CSV
  - üìß MSG (Email)
  - üñ•Ô∏è PPTX (PowerPoint)
  - üìò ODT (OpenDocument Text)
  - üìö EPUB (E-book)
  - üíª Code files (PY, JS, JAVA, TS, TSX, C, CPP, H, and more!)
- **Powerful AI Analysis:** Uses the power of LangChain to provide in-depth analysis.
- **Structured Output:** Get results in a well-defined format using Pydantic.
- **Customizable Prompts:** Tailor the analysis to your specific needs with pre-defined or custom prompts.
- **Tone and Instruction Control:** Fine-tune the LLM's responses by selecting the desired tone (e.g., professional, informal, academic) and specific instructions (e.g., act as a researcher, software engineer, business analyst).
- **Length/Detail Selection:** Control the length and level of detail of the generated responses (e.g., concise, detailed, comprehensive).
- **Flexible Analysis Modes:** Choose to analyze each document individually or combine them for a holistic analysis.
- **Interactive Chat:** Continue the conversation with the LLM to explore the documents further.
- **Docker Support:** Easily deploy the application using Docker or Docker Compose.

## üìñ Table of Contents

1. [Features of DocMind AI](#-features-of-docmind-ai)
2. [Getting Started with DocMind AI: Local LLM Analysis](#-getting-started-with-docmind-ai-local-llm-analysis)
   - [Prerequisites](#-prerequisites)
   - [Installation](#Ô∏è-installation)
   - [Running the App](#Ô∏è-running-the-app)
3. [Usage](#-usage)
   - [Selecting a Model](#Ô∏è-selecting-a-model)
   - [Uploading Documents](#-uploading-documents)
   - [Choosing Prompts](#Ô∏è-choosing-prompts)
   - [Selecting Tone](#-selecting-tone)
   - [Selecting Instructions](#-selecting-instructions)
   - [Setting Length/Detail](#-setting-lengthdetail)
   - [Choosing Analysis Mode](#Ô∏è-choosing-analysis-mode)
   - [Analyzing Documents](#-analyzing-documents)
   - [Interacting with the LLM](#-interacting-with-the-llm)
4. [Architecture](#Ô∏è-architecture)
5. [How to Cite](#-how-to-cite)
6. [Contributing](#-contributing)
7. [License](#-license)

## üöÄ Getting Started with DocMind AI: Local LLM Analysis

### üìã Prerequisites

- [Ollama](https://ollama.com/) installed and running.
- Python 3.8 or higher.
- (Optional) Docker and Docker Compose for containerized deployment.

### ‚öôÔ∏è Installation

1. **Clone the repository:**

   ```bash
   git clone https://github.com/BjornMelin/docmind-ai.git
   cd docmind-ai
   ```

2. **Install dependencies:**

   ```bash
   pip install -r requirements.txt
   ```

### ‚ñ∂Ô∏è Running the App

**Locally:**

```bash
streamlit run app.py
```

**With Docker:**

```bash
docker-compose up --build
```

The app will be accessible at `http://localhost:8501`.

## üíª Usage

### üéõÔ∏è Selecting a Model

1. Enter the **Ollama Base URL** (default: `http://localhost:11434`).
2. Choose your desired **Ollama Model Name** (e.g., `llama2`) from the dropdown.

### üìÅ Uploading Documents

Click the **"Browse files"** button to upload one or more documents. Supported file types are listed above in the [Features](#-features) section.

### ‚úçÔ∏è Choosing Prompts

Select a pre-defined prompt from the dropdown:

- **Comprehensive Document Analysis:** Get a summary, key insights, action items, and open questions.
- **Extract Key Insights and Action Items:** Focus on extracting these two elements.
- **Summarize and Identify Open Questions:** Generate a summary and a list of open questions.
- **Custom Prompt:** Enter your own prompt to guide the analysis.

### üòÉ Selecting Tone

Choose the desired tone for the LLM's response:

- **Professional:** Objective and formal.
- **Academic:** Scholarly and research-oriented.
- **Informal:** Casual and conversational.
- **Creative:** Imaginative and artistic.
- **Neutral:** Unbiased and objective.
- **Direct:** Concise and to-the-point.
- **Empathetic:** Understanding and compassionate.
- **Humorous:** Witty and lighthearted.
- **Authoritative:** Confident and expert.
- **Inquisitive:** Curious and exploratory.

### üßÆ Selecting Instructions

Select the persona or instructions that the LLM should follow:

- **General Assistant:** Act as a helpful assistant.
- **Researcher:** Provide in-depth research and analysis.
- **Software Engineer:** Focus on technical details and code.
- **Product Manager:** Consider product strategy and user experience.
- **Data Scientist:** Emphasize data analysis and modeling.
- **Business Analyst:** Analyze from a business and strategic perspective.
- **Technical Writer:** Create clear and concise documentation.
- **Marketing Specialist:** Focus on branding and customer engagement.
- **HR Manager:** Consider human resources aspects.
- **Legal Advisor:** Provide information from a legal standpoint.
- **Custom Instructions:** Enter your own specific instructions.

### üìè Setting Length/Detail

Choose the desired length and level of detail for the LLM's response:

- **Concise:** Brief and to-the-point.
- **Detailed:** Thorough and comprehensive.
- **Comprehensive:** Extensive and in-depth.
- **Bullet Points:** Provide response in bullet point format.

### üóÇÔ∏è Choosing Analysis Mode

Select the analysis mode:

- **Analyze each document separately:** Process and analyze each document individually.
- **Combine analysis for all documents:** Treat all uploaded documents as a single unit for analysis.

### üß† Analyzing Documents

1. Upload your documents.
2. Choose your analysis prompt, tone, instructions, desired length, and analysis mode.
3. Click the **"Extract and Analyze"** button.

The application will display the analysis results, attempting to format them according to the defined output schema. If parsing fails, the raw LLM output will be shown.

### üí¨ Interacting with the LLM

Use the chat interface to ask follow-up questions about the analyzed documents. The LLM will use the extracted information as context for its responses.

## üèóÔ∏è Architecture

Here's a Mermaid diagram illustrating the application's architecture:

```mermaid
graph TD
    A[User] -->|Uploads Documents| B(Streamlit App - app.py);
    B -->|Selects Model, Prompt, Tone, Instructions, Length, Mode| C{Ollama API};
    C -->|Processes Documents| D[LangChain];
    D -->|Loads Documents| E{Document Loaders};
    E -->|PDF| F[PyPDFLoader];
    E -->|DOCX| G[Docx2Loader];
    E -->|TXT, Code| H[TextLoader];
    E -->|...| I;
    D -->|Splits Text| J[RecursiveCharacterTextSplitter];
    D -->|Generates Analysis| K[LLM - Ollama Model];
    K -->|Structured Output| L[PydanticOutputParser];
    B -->|Displays Results| A;
    A -->|Asks Follow-up Questions| B;
    B -->|Interacts with LLM| C;

    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#ccf,stroke:#333,stroke-width:2px
    style C fill:#cfc,stroke:#333,stroke-width:2px
    style D fill:#fcc,stroke:#333,stroke-width:2px
    style K fill:#ccf,stroke:#333,stroke-width:2px
```

## üìñ How to Cite

If you use DocMind AI in your research or work, please cite it as follows:

```bibtex
@software{melin_docmind_ai_2025,
  author = {Melin, Bjorn},
  title = {DocMind AI: Local LLM for AI-Powered Document Analysis},
  url = {https://github.com/BjornMelin/docmind-ai},
  version = {0.1.0},
  year = {2025}
}
```

## üôå Contributing

We welcome contributions! Please see the [CONTRIBUTING.md](CONTRIBUTING.md) file for details on how to contribute to this project.

## üìÉ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

Built with ‚ù§Ô∏è by [Bjorn Melin](https://bjornmelin.io)

</div>
